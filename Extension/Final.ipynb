{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Final.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1ySjtArbXkBE-g5c_RJWNHBJO4CkhHfB4","authorship_tag":"ABX9TyOkPveYiv8pl9wYtWMb9e+9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9yZqldcqTlZ5","executionInfo":{"status":"ok","timestamp":1624275032238,"user_tz":-330,"elapsed":430,"user":{"displayName":"Kriz Moses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5AHG3pEQIY30An0gBgkynBoQrUDYQ22HSAT0p7A=s64","userId":"11504316763430471705"}},"outputId":"899928d9-023f-4c99-94f8-b44b1bdf7c33"},"source":["%cd /content/drive/MyDrive/Home_Depot_Case_Study/Workspace_Deployment\n","!pwd"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/Home_Depot_Case_Study/Workspace_Deployment\n","/content/drive/MyDrive/Home_Depot_Case_Study/Workspace_Deployment\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yDLarWmTbLPI"},"source":["## Imports and Loadings"]},{"cell_type":"code","metadata":{"id":"PC90DMJ0amjN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624275043240,"user_tz":-330,"elapsed":10320,"user":{"displayName":"Kriz Moses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5AHG3pEQIY30An0gBgkynBoQrUDYQ22HSAT0p7A=s64","userId":"11504316763430471705"}},"outputId":"1aed8f35-0163-4a12-a88d-1ff11a4ca402"},"source":["import pandas as pd \n","import numpy as np \n","import regex as re\n","!pip install nltk \n","import nltk\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","from collections import Counter\n","from nltk.stem import PorterStemmer \n","from wordcloud import STOPWORDS \n","from prettytable import PrettyTable\n","import warnings\n","warnings.filterwarnings('ignore')\n","import math\n","from nltk import sent_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from gensim.models import Word2Vec\n","from numpy.linalg import norm\n","import pickle\n","from tqdm.notebook import tqdm\n","from scipy.stats import uniform, randint, loguniform\n","!pip install rank_bm25"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","Collecting rank_bm25\n","  Downloading https://files.pythonhosted.org/packages/16/5a/23ed3132063a0684ea66fb410260c71c4ffda3b99f8f1c021d1e245401b5/rank_bm25-0.2.1-py3-none-any.whl\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rank_bm25) (1.19.5)\n","Installing collected packages: rank-bm25\n","Successfully installed rank-bm25-0.2.1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9UcPxe4htzAr"},"source":["#### BM25"]},{"cell_type":"code","metadata":{"id":"q6Lx60Qrt38w"},"source":["database  = pd.read_csv('database.csv')\n","product_text = database['text'].values\n","with open('BM25_model.pkl', 'rb') as f:\n","  bm25_model = pickle.load(f)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4ub07ETSt07_"},"source":["#### LTOR"]},{"cell_type":"markdown","metadata":{"id":"OuUhNdJMdeV9"},"source":["**CLEANING**"]},{"cell_type":"code","metadata":{"id":"2obCBgaEbQ5Z"},"source":["with open('Final/cleaning/unique_brands.pkl','rb') as f:\n","  unique_brands = pickle.load(f)\n","\n","def first_n(n, sent):\n","  if n > len(sent.split()):\n","    return 'error101'\n","  return ' '.join(sent.split()[:n])\n","\n","def fillna_brand(data, unique_brnds):\n","  null_df = data[data['brand'].isnull()]\n","  notnull_df = data.dropna()\n","\n","  for i, row in null_df.iterrows():\n","    title = row['product_title']\n","    if first_n(4, title) in unique_brnds:\n","      null_df['brand'].loc[i] = first_n(4, title)\n","    elif first_n(3, title) in unique_brnds:\n","      null_df['brand'].loc[i] = first_n(3, title)\n","    elif first_n(2, title) in unique_brnds:\n","      null_df['brand'].loc[i] = first_n(2, title)\n","    else:\n","      null_df['brand'].loc[i] = first_n(1, title)\n","\n","  data['brand'].loc[null_df.index] = null_df['brand'].values\n","  return data\n","\n","def fillna_attributes(data):\n","  null_df = data[data['combined_attr'].isnull()]\n","  null_df['combined_attr'] = null_df['product_description'].copy()\n","  data['combined_attr'].loc[null_df.index] = null_df['combined_attr'].values\n","  return data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_rO19Y1hb8kP"},"source":["def standardize_units(text):\n","  text = \" \" + text + \" \"\n","  text = re.sub('( gal | gals | galon )',' gallon ',text)\n","  text = re.sub('( ft | fts | feets | foot | foots )',' feet ',text)\n","  text = re.sub('( squares | sq )',' square ',text)\n","  text = re.sub('( lb | lbs | pounds )',' pound ',text)\n","  text = re.sub('( oz | ozs | ounces | ounc )',' ounce ',text)\n","  text = re.sub('( yds | yd | yards )',' yard ',text)\n","  return text\n","\n","def preprocessing(sent):\n","  sent = sent.replace('in.', ' inch ') #If we dont to this then 'in.' will be turned to 'in' in the next step\n","  words = re.split(r'\\W+', sent)\n","  words = [word.lower() for word in words]\n","  res = re.sub(\"[A-Za-z]+\", lambda ele: \" \" + ele[0] + \" \", ' '.join(words)) #add space between number and alphabets in a string\n","  cleaned = standardize_units(res) \n","  cleaned = ' '.join(cleaned.split()) #removing extra whitespaces\n","  return cleaned\n","\n","def preprocessing_search(sent):\n","  sent = sent.replace('in.', ' inch ')\n","  words = re.split(r'\\W+', sent)\n","  words = [word.lower() for word in words]\n","  res = re.sub(\"[A-Za-z]+\", lambda ele: \" \" + ele[0] + \" \", ' '.join(words)) #add space between number and alphabets in a string\n","  res = standardize_units(res) \n","  res = res.replace(' in ', ' inch ') #in search_terms 'in' is used more for 'inch' than as a preposition hence this step shouldn't hurt\n","  cleaned = ' '.join(res.split()) #removing extra whitespaces\n","  return cleaned"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ORvlQx9hbQY0"},"source":["def words(text): return re.findall(r'\\w+', text.lower())\n","WORDS = Counter(words(open('Final/cleaning/corpus.txt').read()))\n","\n","def P(word, N=sum(WORDS.values())): \n","    \"Probability of `word`.\"\n","    return WORDS[word] / N\n","def correction(word): \n","    \"Most probable spelling correction for word.\"\n","    return max(candidates(word), key=P)\n","def candidates(word): \n","    \"Generate possible spelling corrections for word.\"\n","    return (known([word]) or known(edits1(word)) or known(edits2(word)) or set([word]))\n","def known(words): \n","    \"The subset of `words` that appear in the dictionary of WORDS.\"\n","    return set(w for w in words if w in WORDS)\n","def edits1(word):\n","    \"All edits that are one edit away from `word`.\"\n","    letters    = 'abcdefghijklmnopqrstuvwxyz'\n","    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n","    deletes    = [L + R[1:]               for L, R in splits if R]\n","    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n","    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n","    inserts    = [L + c + R               for L, R in splits for c in letters]\n","    return set(deletes + transposes + replaces + inserts)\n","def edits2(word): \n","    \"All edits that are two edits away from `word`.\"\n","    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n","def corrected_term(term):\n","  temp = term.lower().split()\n","  temp = [correction(word) for word in temp]\n","  return ' '.join(temp)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yZtraXWmcc9t"},"source":["#stop word removal and stemming\n","#We didn't do this before because we wanted to fix the typos in the searh term first \n","porter = PorterStemmer()\n","stp_wrds = set(stopwords.words('english'))\n","\n","def futher_preprocessing(sent):\n","  sent = sent.replace('_', ' _ ')\n","  words = sent.split()\n","  words = [w for w in words if not w in stp_wrds]\n","  words = [porter.stem(word) for word in words]\n","  return ' '.join(words)\n","\n","#stop word removal only - no stemming\n","def futher_preprocessing_without_stem(sent):\n","  sent = sent.replace('_', ' _ ')\n","  words = sent.split()\n","  words = [w for w in words if not w in stp_wrds]\n","  return ' '.join(words)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zRANmdizdgys"},"source":["**FEATURIZATION**"]},{"cell_type":"code","metadata":{"id":"5nRw-5G3d6ll"},"source":["def common_words(df, col1, col2):\n","  \"\"\"\n","  Returns common words between each row of col1 and col2 of df in the form of a list. \n","  Length of list is number of rows in dataframe\n","  \"\"\"\n","  common_list = []\n","  for i, row in df[[col1,col2]].iterrows():\n","    set1 = set(row[col1].split())\n","    set2 = set(row[col2].split())\n","    common = set1 & set2\n","    common = ' '.join(common)\n","    common_list.append(common)\n","  return common_list\n","\n","def cosine_similarity_sent(sent1, sent2):\n","  \"\"\"\n","  Cosine Similarity between 2 sentences treating them as sets of words\n","  \"\"\"\n","  set1 = set(sent1.split())\n","  set2 = set(sent2.split())\n","  numerator = len(set1 & set2)\n","  denominator = math.sqrt(len(set1)) * math.sqrt(len(set2))\n","  if not denominator:\n","      return 0.0\n","  else:\n","      return numerator / denominator\n","\n","def jacquard_coefficient_sent(sent1, sent2):\n","  \"\"\"\n","  Jacquard Coefficient between 2 sentences treating them as sets of words\n","  \"\"\"\n","  set1 = set(sent1.split())\n","  set2 = set(sent2.split())\n","  numerator = len(set1 & set2)\n","  denominator = len(set1 | set2)\n","  if not denominator:\n","      return 0.0\n","  else:\n","      return numerator / denominator"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UiEOJlltd4Rs"},"source":["corrected_feat_set = ['num_common_ST','num_common_SD', 'num_common_SB', 'cosine_ST', 'cosine_SD', \n","                'cosine_SB', 'jacquard_ST', 'jacquard_SD', 'jacquard_SB', \n","                'len_description', 'len_brand', 'len_title', 'len_search',\n","                'islast_ST', 'islast_SD', 'islast_SB']\n","\n","raw_feat_set = ['num_common_r_ST', 'num_common_r_SD', 'num_common_r_SB', 'cosine_r_ST', 'cosine_r_SD', \n","                'cosine_r_SB',  'jacquard_r_ST', 'jacquard_r_SD', 'jacquard_r_SB', \n","                'len_description', 'len_brand', 'len_title', 'len_r_search', \n","                'islast_r_ST','islast_r_SD', 'islast_r_SB']\n","\n","feat_set1_comb =  ['num_common_ST','num_common_SD', 'num_common_SB', 'cosine_ST', 'cosine_SD', \n","                'cosine_SB', 'jacquard_ST', 'jacquard_SD', 'jacquard_SB', \n","                'len_description', 'len_brand', 'len_title', 'len_search',\n","                'islast_ST', 'islast_SD', 'islast_SB', 'num_common_r_ST', 'num_common_r_SD', 'num_common_r_SB', 'cosine_r_ST', 'cosine_r_SD', \n","                'cosine_r_SB',  'jacquard_r_ST', 'jacquard_r_SD', 'jacquard_r_SB', \n","                'len_r_search', 'islast_r_ST','islast_r_SD', 'islast_r_SB']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fhj9Y65Je784"},"source":["def cosine_similarity_vec(a, b):\n","  \"\"\"\n","  Cosine Similarity between 2 vectors\n","  \"\"\"  \n","  num = np.dot(a, b)\n","  den = norm(a)*norm(b)\n","  if den != 0:\n","    return num/den\n","  else:\n","    return 0\n","\n","def jacquard_similarity_vec(a, b):\n","  \"\"\"\n","  Cosine Similarity between 2 vectors\n","  \"\"\"  \n","  num = np.dot(a,b)\n","  den = norm(a)**2 + norm(b)**2 - np.dot(a,b)\n","  if den != 0:\n","    return num/den\n","  else:\n","    return 0\n","\n","def inner_product_vec(a, b):\n","  return np.dot(a,b)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y-AcPKuai-3G"},"source":["with open('Final/featurization/F2_tfidf_search.pkl','rb') as f:\n","  F2_tfidf_search = pickle.load(f)\n","with open('Final/featurization/F2_tsvd_search.pkl','rb') as f:\n","  F2_tsvd_search = pickle.load(f)\n","\n","with open('Final/featurization/F2_tfidf_title.pkl','rb') as f:\n","  F2_tfidf_title = pickle.load(f)\n","with open('Final/featurization/F2_tsvd_title.pkl','rb') as f:\n","  F2_tsvd_title = pickle.load(f)\n","\n","with open('Final/featurization/F2_tfidf_desc.pkl','rb') as f:\n","  F2_tfidf_desc = pickle.load(f)\n","with open('Final/featurization/F2_tsvd_desc.pkl','rb') as f:\n","  F2_tsvd_desc = pickle.load(f)\n","\n","with open('Final/featurization/F2_tfidf_lsi.pkl','rb') as f:\n","  F2_tfidf_lsi = pickle.load(f)\n","with open('Final/featurization/F2_tsvd_lsi.pkl','rb') as f:\n","  F2_tsvd_lsi = pickle.load(f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vXXBxqH_kbPq"},"source":["def lmir_fit(corpus):\n","  words = ' '.join(corpus).split()\n","  freq_dict = Counter(words)\n","  total_words = len(words)\n","  params = {\n","      'freq_dict':freq_dict,\n","      'total_words':total_words\n","  }\n","  return params\n","\n","def lmir_jm_score(query, doc, params, lambd):\n","  query = query.split()\n","  doc = doc.split()\n","  if len(doc) != 0 and len(query) != 0:\n","    eps = 0.0001/(params['total_words'])\n","    score = 0\n","    for word in query:\n","      p_ml = doc.count(word) / len(doc)\n","      if word in params['freq_dict'].keys():\n","        p_c = params['freq_dict'][word] / params['total_words']\n","      else: \n","        p_c = 0\n","      score += np.log(lambd*p_ml + (1-lambd)*p_c + eps)\n","    return score\n","\n","def lmir_dir_score(query, doc, params, mu):\n","  query = query.split()\n","  doc = doc.split()\n","  if len(doc) != 0 and len(query) != 0:\n","    eps = 0.0001/(params['total_words'])\n","    score = 0\n","    for word in query:\n","      p_ml = doc.count(word) / len(doc)\n","      if word in params['freq_dict']:\n","        p_c = params['freq_dict'][word] / params['total_words']\n","      else: \n","        p_c = 0\n","      lambd = len(doc) / (len(doc) + mu)\n","      score += np.log(lambd*p_ml + (1-lambd)*p_c + eps)\n","    return score\n","\n","def lmir_abs_score(query, doc, alpha):\n","  query = query.split()\n","  doc = doc.split()\n","  if len(doc) != 0 and len(query) != 0:\n","    score = 0\n","    temp_dict = {k:v+alpha for k,v in Counter(doc).items()}\n","    for word in query:\n","      if word in temp_dict:\n","        pass\n","      else:\n","        temp_dict[word] = alpha\n","    denominator = sum(temp_dict.values())\n","    for word in query:\n","      score += temp_dict[word] / denominator\n","  return score\n","\n","with open('Final/featurization/F3_LM_params_title.pkl','rb') as f:\n","  params_title_LM = pickle.load(f)\n","with open('Final/featurization/F3_LM_params_brand.pkl','rb') as f:\n","  params_brand_LM = pickle.load(f)\n","with open('Final/featurization/F3_LM_params_desc.pkl','rb') as f:\n","  params_desc_LM = pickle.load(f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zBn9p-G9lMd2"},"source":["def bm25_fit(corpus):\n","  tfidf_model = TfidfVectorizer(smooth_idf=False, token_pattern=r\"(?u)\\b\\w+\\b\")\n","  tfidf_model.fit(corpus)\n","  idf_dict = dict(zip(tfidf_model.get_feature_names(), list(tfidf_model.idf_)))\n","  avgdl = np.mean([len(doc.split()) for doc in corpus])\n","  params = {'idf_dict': idf_dict, \n","            'avgdl' : avgdl,\n","            'N' : N}\n","  return params\n","\n","def bm25_score(query, doc, params, k=0.1, b=0.5):\n","  idf_dict = params['idf_dict']\n","  avgdl = params['avgdl']\n","  N = params['N']\n","  score_query = 0\n","  for word in query.split():\n","    dl = len(doc.split())\n","    tf = doc.count(word)\n","    if word in idf_dict.keys():\n","      idf = idf_dict[word]\n","    else: \n","      idf = np.log(N+1) \n","    score_word = idf*(tf*(k+1))/(tf + k*(1-b) + b*dl/avgdl)\n","    score_query += score_word\n","  return score_query\n","\n","with open('Final/featurization/F3_bm25_params_title.pkl','rb') as f:\n","  params_title_bm25 = pickle.load(f)\n","with open('Final/featurization/F3_bm25_params_desc.pkl','rb') as f:\n","  params_desc_bm25 = pickle.load(f)\n","with open('Final/featurization/F3_bm25_params_brand.pkl','rb') as f:\n","  params_brand_bm25 = pickle.load(f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-DzJ3h-xl0nu"},"source":["with open('glove_vectors', 'rb') as f:\n","    model = pickle.load(f)\n","    glove_words =  set(model.keys())\n","\n","with open('Final/featurization/F3_tfidf_w2v_params_search.pkl','rb') as f:\n","  tfidf_w2v_params_search = pickle.load(f)\n","with open('Final/featurization/F3_tfidf_w2v_params_title.pkl','rb') as f:\n","  tfidf_w2v_params_title = pickle.load(f)\n","with open('Final/featurization/F3_tfidf_w2v_params_desc.pkl','rb') as f:\n","  tfidf_w2v_params_desc = pickle.load(f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sDGERC9Ln-44"},"source":["with open('Final/featurization/F3_SmM_params_title.pkl','rb') as f:\n","  F3_SmM_params_title = pickle.load(f)\n","with open('Final/featurization/F3_SmM_params_brand.pkl','rb') as f:\n","  F3_SmM_params_brand = pickle.load(f)\n","with open('Final/featurization/F3_SmM_params_desc.pkl','rb') as f:\n","  F3_SmM_params_desc = pickle.load(f)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7GMIPMDEpMy-"},"source":["**MODELLING**"]},{"cell_type":"code","metadata":{"id":"JwSCXo2ppRSh"},"source":["F1_scaler_ridge = pickle.load(open('Final/modelling/base_models/F1_scaler_ridge.pkl', 'rb'))\n","F1_scaler_lasso = pickle.load(open('Final/modelling/base_models/F1_scaler_lasso.pkl', 'rb'))\n","F1_scaler_en = pickle.load(open('Final/modelling/base_models/F1_scaler_en.pkl', 'rb'))\n","F1_scaler_svr = pickle.load(open('Final/modelling/base_models/F1_scaler_svr.pkl', 'rb'))\n","\n","F1_xgb = pickle.load(open('Final/modelling/base_models/F1_xgb.pkl', 'rb'))\n","F1_rf = pickle.load(open('Final/modelling/base_models/F1_rf.pkl', 'rb'))\n","F1_ridge = pickle.load(open('Final/modelling/base_models/F1_ridge.pkl', 'rb'))\n","F1_lasso = pickle.load(open('Final/modelling/base_models/F1_lasso.pkl', 'rb'))\n","F1_en = pickle.load(open('Final/modelling/base_models/F1_en.pkl', 'rb'))\n","F1_svr = pickle.load(open('Final/modelling/base_models/F1_svr.pkl', 'rb'))\n","F1_dt = pickle.load(open('Final/modelling/base_models/F1_dt.pkl', 'rb'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jp_TfVfbpm14"},"source":["#Loading the standard scalers\n","F2_scaler_ridge = pickle.load(open('Final/modelling/base_models/F2_scaler_ridge.pkl', 'rb'))\n","F2_scaler_lasso = pickle.load(open('Final/modelling/base_models/F2_scaler_lasso.pkl', 'rb'))\n","F2_scaler_en = pickle.load(open('Final/modelling/base_models/F2_scaler_en.pkl', 'rb'))\n","F2_scaler_svr = pickle.load(open('Final/modelling/base_models/F2_scaler_svr.pkl', 'rb'))\n","\n","#Loading the models \n","F2_svr = pickle.load(open('Final/modelling/base_models/F2_svr.pkl', 'rb'))\n","F2_rf = pickle.load(open('Final/modelling/base_models/F2_rf.pkl', 'rb'))\n","F2_ridge = pickle.load(open('Final/modelling/base_models/F2_ridge.pkl', 'rb'))\n","F2_lasso = pickle.load(open('Final/modelling/base_models/F2_lasso.pkl', 'rb'))\n","F2_en = pickle.load(open('Final/modelling/base_models/F2_en.pkl', 'rb'))\n","F2_dt = pickle.load(open('Final/modelling/base_models/F2_dt.pkl', 'rb'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jwku4HRApsGa"},"source":["#Loading the standard scalers\n","F3_scaler_en = pickle.load(open('Final/modelling/base_models/F3_scaler_en.pkl', 'rb'))\n","F3_scaler_ridge = pickle.load(open('Final/modelling/base_models/F3_scaler_ridge.pkl', 'rb'))\n","F3_scaler_lasso = pickle.load(open('Final/modelling/base_models/F3_scaler_lasso.pkl', 'rb'))\n","\n","#Loading the models \n","F3_ridge = pickle.load(open('Final/modelling/base_models/F3_ridge.pkl', 'rb'))\n","F3_lasso = pickle.load(open('Final/modelling/base_models/F3_lasso.pkl', 'rb'))\n","F3_en = pickle.load(open('Final/modelling/base_models/F3_en.pkl', 'rb'))\n","F3_dt = pickle.load(open('Final/modelling/base_models/F3_dt.pkl', 'rb'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J1QNKmyOqCL4"},"source":["final_scaler = pickle.load(open('Final/modelling/meta_scaler.pkl', 'rb'))\n","final_ridge = pickle.load(open('Final/modelling/meta_ridge.pkl', 'rb'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qzL21q1bhdhb"},"source":["## Final Functions"]},{"cell_type":"code","metadata":{"id":"HRkfSn71aggT"},"source":["def get_search_relevance(test_set):\n","  test_set = fillna_brand(test_set, unique_brands)\n","  test_set = fillna_attributes(test_set)\n","  test_set = test_set.fillna('')\n","\n","  test_set['cleaned_title'] = test_set['product_title'].apply(lambda x : preprocessing(x))\n","  test_set['cleaned_brand'] = test_set['brand'].apply(lambda x : preprocessing(x))\n","  test_set['cleaned_description'] = test_set['product_description'].apply(lambda x : preprocessing(x))\n","  test_set['cleaned_attributes'] = test_set['combined_attr'].apply(lambda x : preprocessing(x))\n","  test_set['cleaned_search'] = test_set['search_term'].apply(lambda x : preprocessing_search(x))\n","\n","  test_set['corrected_search'] = test_set['cleaned_search'].apply(lambda x: corrected_term(x))\n","\n","  cleaned_test_set = pd.DataFrame() \n","  cleaned_test_set['title'] = test_set['cleaned_title'].apply(lambda x : futher_preprocessing(x)) \n","  cleaned_test_set['brand'] = test_set['cleaned_brand'].apply(lambda x : futher_preprocessing(x)) \n","  cleaned_test_set['description'] = test_set['cleaned_description'].apply(lambda x : futher_preprocessing(x)) \n","  cleaned_test_set['attributes'] = test_set['cleaned_attributes'].apply(lambda x : futher_preprocessing(x)) \n","  cleaned_test_set['search'] = test_set['cleaned_search'].apply(lambda x : futher_preprocessing(x)) \n","  cleaned_test_set['corrected_search'] = test_set['corrected_search'].apply(lambda x : futher_preprocessing(x)) \n","\n","  cleaned_test_set2 = pd.DataFrame()\n","  cleaned_test_set2['title'] = test_set['cleaned_title'].apply(lambda x : futher_preprocessing_without_stem(x)) \n","  cleaned_test_set2['brand'] = test_set['cleaned_brand'].apply(lambda x : futher_preprocessing_without_stem(x)) \n","  cleaned_test_set2['description'] = test_set['cleaned_description'].apply(lambda x : futher_preprocessing_without_stem(x))\n","  cleaned_test_set2['attributes'] = test_set['cleaned_attributes'].apply(lambda x : futher_preprocessing_without_stem(x)) \n","  cleaned_test_set2['search'] = test_set['cleaned_search'].apply(lambda x : futher_preprocessing_without_stem(x)) \n","  cleaned_test_set2['corrected_search'] = test_set['corrected_search'].apply(lambda x : futher_preprocessing_without_stem(x)) \n","\n","  cleaned_test_set['brand'] = cleaned_test_set['brand'].replace(to_replace =[\"\"], value =\"missing_brand\")\n","  cleaned_test_set2['brand'] = cleaned_test_set2['brand'].replace(to_replace =[\"\"], value =\"missing_brand\")\n","  cleaned_test_set['search'] = cleaned_test_set['search'].replace(to_replace =[\"\"], value =\"missing_search\")\n","  cleaned_test_set2['search'] = cleaned_test_set2['search'].replace(to_replace =[\"\"], value =\"missing_search\")\n","\n","  cleaned_test_set['attributes'] = cleaned_test_set['attributes'].apply(lambda x: re.sub('bullet \\d\\d ', '', x))\n","  cleaned_test_set2['attributes'] = cleaned_test_set2['attributes'].apply(lambda x: re.sub('bullet \\d\\d ', '', x))\n","\n","  cleaned_test_set['description'] = cleaned_test_set['description'].apply(lambda x: re.sub('bullet \\d\\d ', '', x))\n","  cleaned_test_set2['description'] = cleaned_test_set2['description'].apply(lambda x: re.sub('bullet \\d\\d ', '', x))\n","\n","  cleaned_test_set.rename(columns={\"search\": \"raw_search\"}, inplace=True)\n","  cleaned_test_set2.rename(columns={\"search\": \"raw_search\"}, inplace=True)\n","\n","  #FEATURIZATION\n","  #set1\n","  data1 = cleaned_test_set.copy()\n","\n","  data1['common_ST'] = common_words(data1,'corrected_search', 'title')\n","  data1['common_SD'] = common_words(data1,'corrected_search', 'description')\n","  data1['common_SB'] = common_words(data1,'corrected_search', 'brand')\n","  data1['common_r_ST'] = common_words(data1,'raw_search', 'title')\n","  data1['common_r_SD'] = common_words(data1,'raw_search', 'description')\n","  data1['common_r_SB'] = common_words(data1,'raw_search', 'brand')\n","\n","  data1['num_common_ST'] = data1['common_ST'].apply(lambda x : len(x.split()))\n","  data1['num_common_SD'] = data1['common_SD'].apply(lambda x : len(x.split()))\n","  data1['num_common_SB'] = data1['common_SB'].apply(lambda x : len(x.split()))\n","  data1['num_common_r_ST'] = data1['common_r_ST'].apply(lambda x : len(x.split()))\n","  data1['num_common_r_SD'] = data1['common_r_SD'].apply(lambda x : len(x.split()))\n","  data1['num_common_r_SB'] = data1['common_r_SB'].apply(lambda x : len(x.split()))\n","\n","  data1['cosine_ST'] = data1.apply(lambda row: cosine_similarity_sent(row['corrected_search'], row['title']), axis=1) \n","  data1['cosine_SD'] = data1.apply(lambda row: cosine_similarity_sent(row['corrected_search'], row['description']), axis=1)\n","  data1['cosine_SB'] = data1.apply(lambda row: cosine_similarity_sent(row['corrected_search'], row['brand']), axis=1)\n","  data1['cosine_r_ST'] = data1.apply(lambda row: cosine_similarity_sent(row['raw_search'], row['title']), axis=1) \n","  data1['cosine_r_SD'] = data1.apply(lambda row: cosine_similarity_sent(row['raw_search'], row['description']), axis=1)\n","  data1['cosine_r_SB'] = data1.apply(lambda row: cosine_similarity_sent(row['raw_search'], row['brand']), axis=1)\n","\n","  data1['jacquard_ST'] = data1.apply(lambda row: jacquard_coefficient_sent(row['corrected_search'], row['title']), axis=1) \n","  data1['jacquard_SD'] = data1.apply(lambda row: jacquard_coefficient_sent(row['corrected_search'], row['description']), axis=1)\n","  data1['jacquard_SB'] = data1.apply(lambda row: jacquard_coefficient_sent(row['corrected_search'], row['brand']), axis=1)\n","  data1['jacquard_r_ST'] = data1.apply(lambda row: jacquard_coefficient_sent(row['raw_search'], row['title']), axis=1) \n","  data1['jacquard_r_SD'] = data1.apply(lambda row: jacquard_coefficient_sent(row['raw_search'], row['description']), axis=1)\n","  data1['jacquard_r_SB'] = data1.apply(lambda row: jacquard_coefficient_sent(row['raw_search'], row['brand']), axis=1)\n","\n","  data1['len_description'] = data1['description'].apply(lambda x : len(x.split()))\n","  data1['len_brand'] = data1['brand'].apply(lambda x : len(x.split()))\n","  data1['len_title'] = data1['title'].apply(lambda x : len(x.split()))\n","  data1['len_search'] = data1['corrected_search'].apply(lambda x : len(x.split()))\n","  data1['len_r_search'] = data1['raw_search'].apply(lambda x : len(x.split()))\n","\n","  data1['islast_ST'] = data1.apply(lambda row: row['corrected_search'].split()[-1] in row['title'].split(), axis=1)\n","  data1['islast_SD'] = data1.apply(lambda row: row['corrected_search'].split()[-1] in row['description'].split(), axis=1)\n","  data1['islast_SB'] = data1.apply(lambda row: row['corrected_search'].split()[-1] in row['brand'].split(), axis=1)\n","  data1['islast_r_ST'] = data1.apply(lambda row: row['raw_search'].split()[-1] in row['title'].split(), axis=1)\n","  data1['islast_r_SD'] = data1.apply(lambda row: row['raw_search'].split()[-1] in row['description'].split(), axis=1)\n","  data1['islast_r_SB'] = data1.apply(lambda row: row['raw_search'].split()[-1] in row['brand'].split(), axis=1)\n","\n","  bool_cols = ['islast_ST', 'islast_SD', 'islast_SB', 'islast_r_ST', 'islast_r_SD', 'islast_r_SB']\n","  for col in bool_cols:\n","    data1[col] = data1[col].astype(int)\n","\n","  #set2\n","  data2 = cleaned_test_set.copy()\n","\n","  X_search = F2_tfidf_search.transform(data2['corrected_search'])\n","  truncated_search = F2_tsvd_search.transform(X_search)\n","  X_title = F2_tfidf_title.transform(data2['title'])\n","  truncated_title = F2_tsvd_title.transform(X_title)\n","  X_desc = F2_tfidf_desc.transform(data2['description'])\n","  truncated_desc = F2_tsvd_desc.transform(X_desc)\n","  trun_arr = np.hstack((truncated_search,truncated_title,truncated_desc))\n","  truncated_df = pd.DataFrame(trun_arr, index=cleaned_test_set.index)\n","\n","  title_desc = data2[\"title\"].astype(str) + ' ' + data2[\"description\"].astype(str)\n","  X_title_desc = F2_tfidf_lsi.transform(title_desc)\n","  truncated_title_desc = F2_tsvd_lsi.transform(X_title_desc)\n","  X_search_ = F2_tfidf_lsi.transform(data2['corrected_search'])\n","  transformed_search = F2_tsvd_lsi.transform(X_search_)\n","\n","  cos_sim = []\n","  for i in range(len(transformed_search)):\n","    cos_sim.append(cosine_similarity_vec(truncated_title_desc[i], transformed_search[i]))\n","  data2['lsi_cos_sim'] = cos_sim\n","  jaq_sim = []\n","  for i in range(len(transformed_search)):\n","    jaq_sim.append(jacquard_similarity_vec(truncated_title_desc[i], transformed_search[i]))\n","  data2['lsi_jaq_sim'] = jaq_sim\n","  inn_prod = []\n","  for i in range(len(transformed_search)):\n","    inn_prod.append(inner_product_vec(truncated_title_desc[i], transformed_search[i]))\n","  data2['lsi_inn_prod'] = inn_prod\n","\n","  data2 = data2[['lsi_cos_sim', 'lsi_jaq_sim',\t'lsi_inn_prod']]\n","\n","  #set3\n","  data3 = cleaned_test_set.copy()\n","\n","  data3['JM_ST'] = data3.apply(lambda row: lmir_jm_score(row['corrected_search'], row['title'], params_title_LM, 0.9), axis=1)\n","  data3['Dir_ST'] = data3.apply(lambda row: lmir_dir_score(row['corrected_search'], row['title'], params_title_LM, 12 ), axis=1)\n","  data3['AD_ST'] = data3.apply(lambda row: lmir_abs_score(row['corrected_search'], row['title'], 0.01 ), axis=1)\n","  data3['JM_SB'] = data3.apply(lambda row: lmir_jm_score(row['corrected_search'], row['brand'], params_brand_LM, 0.9), axis=1)\n","  data3['Dir_SB'] = data3.apply(lambda row: lmir_dir_score(row['corrected_search'], row['brand'], params_brand_LM, 1.5 ), axis=1)\n","  data3['AD_SB'] = data3.apply(lambda row: lmir_abs_score(row['corrected_search'], row['brand'], 0.01 ), axis=1)\n","  data3['JM_SD'] = data3.apply(lambda row: lmir_jm_score(row['corrected_search'], row['description'], params_desc_LM, 0.9), axis=1)\n","  data3['Dir_SD'] = data3.apply(lambda row: lmir_dir_score(row['corrected_search'], row['description'], params_desc_LM, 106 ), axis=1)\n","  data3['AD_SD'] = data3.apply(lambda row: lmir_abs_score(row['corrected_search'], row['description'], 0.01 ), axis=1)\n","\n","  data3['bm25_ST'] = data3.apply(lambda row: bm25_score(row['corrected_search'], row['title'], params_title_bm25 ), axis=1)\n","  data3['bm25_SD'] = data3.apply(lambda row: bm25_score(row['corrected_search'], row['description'], params_desc_bm25 ), axis=1)\n","  data3['bm25_SB'] = data3.apply(lambda row: bm25_score(row['corrected_search'], row['brand'], params_brand_bm25 ), axis=1)\n","\n","\n","  dictionary = tfidf_w2v_params_search['dictionary']\n","  tfidf_words = tfidf_w2v_params_search['tfidf_words']\n","  search_tfidf_w2v_test = []; # the avg-w2v for each sentence/review is stored in this list\n","  for sentence in cleaned_test_set2['corrected_search']: # for each review/sentence\n","      vector = np.zeros(300) # as word vectors are of zero length\n","      tf_idf_weight =0; # num of words with a valid vector in the sentence/review\n","      for word in sentence.split(): # for each word in a review/sentence\n","          if (word in glove_words) and (word in tfidf_words):\n","              vec = model[word] # getting the vector for each word\n","              # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)/len(sentence.split())))\n","              tf_idf = dictionary[word]*(sentence.count(word)) # getting the tfidf value for each word\n","              vector += (vec * tf_idf) # calculating tfidf weighted w2v\n","              tf_idf_weight += tf_idf\n","      if tf_idf_weight != 0:\n","          vector /= tf_idf_weight\n","      search_tfidf_w2v_test.append(vector)\n","\n","\n","\n","  dictionary = tfidf_w2v_params_title['dictionary']\n","  tfidf_words = tfidf_w2v_params_title['tfidf_words']\n","  title_tfidf_w2v_test = []; # the avg-w2v for each sentence/review is stored in this list\n","  for sentence in cleaned_test_set2['title']: # for each review/sentence\n","      vector = np.zeros(300) # as word vectors are of zero length\n","      tf_idf_weight =0; # num of words with a valid vector in the sentence/review\n","      for word in sentence.split(): # for each word in a review/sentence\n","          if (word in glove_words) and (word in tfidf_words):\n","              vec = model[word] # getting the vector for each word\n","              # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)/len(sentence.split())))\n","              tf_idf = dictionary[word]*(sentence.count(word)) # getting the tfidf value for each word\n","              vector += (vec * tf_idf) # calculating tfidf weighted w2v\n","              tf_idf_weight += tf_idf\n","      if tf_idf_weight != 0:\n","          vector /= tf_idf_weight\n","      title_tfidf_w2v_test.append(vector)\n","\n","\n","  dictionary = tfidf_w2v_params_desc['dictionary']\n","  tfidf_words = tfidf_w2v_params_desc['tfidf_words']\n","  desc_tfidf_w2v_test = []; # the avg-w2v for each sentence/review is stored in this list\n","  for sentence in cleaned_test_set2['description']: # for each review/sentence\n","      vector = np.zeros(300) # as word vectors are of zero length\n","      tf_idf_weight =0; # num of words with a valid vector in the sentence/review\n","      for word in sentence.split(): # for each word in a review/sentence\n","          if (word in glove_words) and (word in tfidf_words):\n","              vec = model[word] # getting the vector for each word\n","              # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)/len(sentence.split())))\n","              tf_idf = dictionary[word]*(sentence.count(word)) # getting the tfidf value for each word\n","              vector += (vec * tf_idf) # calculating tfidf weighted w2v\n","              tf_idf_weight += tf_idf\n","      if tf_idf_weight != 0:\n","          vector /= tf_idf_weight\n","      desc_tfidf_w2v_test.append(vector)\n","\n","  arr1 = np.array(search_tfidf_w2v_test)\n","  arr2 = np.array(title_tfidf_w2v_test)\n","  arr3 = np.array(desc_tfidf_w2v_test)\n","  tfidf_w2v_df = pd.DataFrame(np.hstack((arr1, arr2, arr3)), index=cleaned_test_set2.index)\n","\n","  idf_dict = F3_SmM_params_title['idf_dict']\n","  N = F3_SmM_params_title['N']\n","  max_tf = []\n","  max_idf = []\n","  max_tfidf = []\n","  min_tf = []\n","  min_idf = []\n","  min_tfidf = []\n","  sum_tf = []\n","  sum_idf = []\n","  sum_tfidf = []\n","  for ind, row in cleaned_test_set.iterrows():\n","    search = row['corrected_search']\n","    text = row['title']\n","    tf_vals = []\n","    idf_vals = []\n","    tfidf_vals = []\n","    for word in search.split():\n","      if word in idf_dict.keys():\n","        tf = text.count(word)\n","        idf = idf_dict[word]\n","      else:\n","        tf = text.count(word)\n","        idf = np.log(N+1)\n","      tf_vals.append(tf)\n","      idf_vals.append(idf)\n","      tfidf_vals.append(tf*idf)\n","    max_tf.append(max(tf_vals))\n","    min_tf.append(min(tf_vals))\n","    sum_tf.append(sum(tf_vals))\n","    max_idf.append(max(idf_vals))\n","    min_idf.append(min(idf_vals))\n","    sum_idf.append(sum(idf_vals))\n","    max_tfidf.append(max(tfidf_vals))\n","    min_tfidf.append(min(tfidf_vals))\n","    sum_tfidf.append(sum(tfidf_vals))\n","\n","  data3['max_tf_ST'] = max_tf\n","  data3['max_idf_ST'] = max_idf\n","  data3['max_tfidf_ST'] = max_tfidf\n","  data3['min_tf_ST'] = min_tf\n","  data3['min_idf_ST'] = min_idf\n","  data3['min_tfidf_ST'] = min_tfidf\n","  data3['sum_tf_ST'] = sum_tf\n","  data3['sum_idf_ST'] = sum_idf\n","  data3['sum_tfidf_ST'] = sum_tfidf\n","\n","  idf_dict = F3_SmM_params_brand['idf_dict']\n","  N = F3_SmM_params_brand['N']\n","  max_tf = []\n","  max_idf = []\n","  max_tfidf = []\n","  min_tf = []\n","  min_idf = []\n","  min_tfidf = []\n","  sum_tf = []\n","  sum_idf = []\n","  sum_tfidf = []\n","  for ind, row in cleaned_test_set.iterrows():\n","    search = row['corrected_search']\n","    text = row['brand']\n","    tf_vals = []\n","    idf_vals = []\n","    tfidf_vals = []\n","    for word in search.split():\n","      if word in idf_dict.keys():\n","        tf = text.count(word)\n","        idf = idf_dict[word]\n","      else:\n","        tf = text.count(word)\n","        idf = np.log(N+1)\n","      tf_vals.append(tf)\n","      idf_vals.append(idf)\n","      tfidf_vals.append(tf*idf)\n","    max_tf.append(max(tf_vals))\n","    min_tf.append(min(tf_vals))\n","    sum_tf.append(sum(tf_vals))\n","    max_idf.append(max(idf_vals))\n","    min_idf.append(min(idf_vals))\n","    sum_idf.append(sum(idf_vals))\n","    max_tfidf.append(max(tfidf_vals))\n","    min_tfidf.append(min(tfidf_vals))\n","    sum_tfidf.append(sum(tfidf_vals))\n","\n","  data3['max_tf_SB'] = max_tf\n","  data3['max_idf_SB'] = max_idf\n","  data3['max_tfidf_SB'] = max_tfidf\n","  data3['min_tf_SB'] = min_tf\n","  data3['min_idf_SB'] = min_idf\n","  data3['min_tfidf_SB'] = min_tfidf\n","  data3['sum_tf_SB'] = sum_tf\n","  data3['sum_idf_SB'] = sum_idf\n","  data3['sum_tfidf_SB'] = sum_tfidf\n","\n","  idf_dict = F3_SmM_params_desc['idf_dict']\n","  N = F3_SmM_params_desc['N']\n","  max_tf = []\n","  max_idf = []\n","  max_tfidf = []\n","  min_tf = []\n","  min_idf = []\n","  min_tfidf = []\n","  sum_tf = []\n","  sum_idf = []\n","  sum_tfidf = []\n","  for ind, row in cleaned_test_set.iterrows():\n","    search = row['corrected_search']\n","    text = row['description']\n","    tf_vals = []\n","    idf_vals = []\n","    tfidf_vals = []\n","    for word in search.split():\n","      if word in idf_dict.keys():\n","        tf = text.count(word)\n","        idf = idf_dict[word]\n","      else:\n","        tf = text.count(word)\n","        idf = np.log(N+1)\n","      tf_vals.append(tf)\n","      idf_vals.append(idf)\n","      tfidf_vals.append(tf*idf)\n","    max_tf.append(max(tf_vals))\n","    min_tf.append(min(tf_vals))\n","    sum_tf.append(sum(tf_vals))\n","    max_idf.append(max(idf_vals))\n","    min_idf.append(min(idf_vals))\n","    sum_idf.append(sum(idf_vals))\n","    max_tfidf.append(max(tfidf_vals))\n","    min_tfidf.append(min(tfidf_vals))\n","    sum_tfidf.append(sum(tfidf_vals))\n","\n","  data3['max_tf_SD'] = max_tf\n","  data3['max_idf_SD'] = max_idf\n","  data3['max_tfidf_SD'] = max_tfidf\n","  data3['min_tf_SD'] = min_tf\n","  data3['min_idf_SD'] = min_idf\n","  data3['min_tfidf_SD'] = min_tfidf\n","  data3['sum_tf_SD'] = sum_tf\n","  data3['sum_idf_SD'] = sum_idf\n","  data3['sum_tfidf_SD'] = sum_tfidf\n","\n","  data3 = data3.iloc[:,6:]\n","\n","  #MODELLING\n","  X1 = pd.concat([data1[feat_set1_comb], data2, data3], axis=1)\n","  pred_xgb = F1_xgb.predict(X1) \n","  pred_rf = F1_rf.predict(X1) \n","  pred_ridge = F1_ridge.predict(F1_scaler_ridge.transform(X1)) \n","  pred_lasso = F1_lasso.predict(F1_scaler_lasso.transform(X1)) \n","  pred_en = F1_en.predict(F1_scaler_en.transform(X1)) \n","  pred_svr = F1_svr.predict(F1_scaler_svr.transform(X1)) \n","  pred_dt = F1_dt.predict(X1) \n","  arr = np.hstack((pred_xgb.reshape(-1,1),\n","                  pred_rf.reshape(-1,1), \n","                  pred_dt.reshape(-1,1), \n","                  pred_svr.reshape(-1,1),\n","                  pred_ridge.reshape(-1,1),\n","                  pred_lasso.reshape(-1,1), \n","                  pred_en.reshape(-1,1)))\n","  F1_df = pd.DataFrame(arr, columns=['f1_xgb', 'f1_rf', 'f1_dt', 'f1_svr', 'f1_ridge', 'f1_lasso', 'f1_en'], index=X1.index)\n","\n","  X2 = pd.concat([data1[feat_set1_comb], data2, data3, tfidf_w2v_df], axis=1)\n","  pred_svr = F2_svr.predict(F2_scaler_svr.transform(X2)) \n","  pred_rf = F2_rf.predict(X2) \n","  pred_ridge = F2_ridge.predict(F2_scaler_ridge.transform(X2)) \n","  pred_lasso = F2_lasso.predict(F2_scaler_lasso.transform(X2)) \n","  pred_en = F2_en.predict(F2_scaler_en.transform(X2)) \n","  pred_dt = F2_dt.predict(X2) \n","  arr = np.hstack((pred_svr.reshape(-1,1),\n","                  pred_rf.reshape(-1,1),                                    \n","                  pred_ridge.reshape(-1,1),\n","                  pred_lasso.reshape(-1,1), \n","                  pred_en.reshape(-1,1),\n","                  pred_dt.reshape(-1,1),))\n","  F2_df = pd.DataFrame(arr, columns=['f2_svr', 'f2_rf', 'f2_ridge', 'f2_lasso', 'f2_en', 'f2_dt'], index=X2.index)\n","\n","  X3 = pd.concat([data1[feat_set1_comb], data2, data3, truncated_df], axis=1)\n","  pred_dt = F3_dt.predict(X3) \n","  pred_ridge = F3_ridge.predict(F3_scaler_ridge.transform(X3)) \n","  pred_lasso = F3_lasso.predict(F3_scaler_lasso.transform(X3)) \n","  pred_en = F3_en.predict(F3_scaler_en.transform(X3)) \n","  arr = np.hstack((pred_dt.reshape(-1,1), \n","                  pred_ridge.reshape(-1,1),\n","                  pred_lasso.reshape(-1,1), \n","                  pred_en.reshape(-1,1)))\n","  F3_df = pd.DataFrame(arr, columns=['f3_dt', 'f3_ridge', 'f3_lasso', 'f3_en'], index=X3.index)\n","\n","  #FINAL\n","  test_x = pd.concat([F1_df, F2_df, F3_df], axis=1)\n","  test_x_std = final_scaler.transform(test_x)\n","  test_y_pred = final_ridge.predict(test_x_std)\n","\n","  return test_y_pred"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"emu8l_4LzuYL"},"source":["def get_candidates(search, N):\n","  search = preprocessing_search(search)\n","  search = corrected_term(search)\n","  tokenized_query = search.split(\" \")\n","  candidates = bm25_model.get_top_n(tokenized_query, product_text, n=N)\n","  candidate_products = database[database['text'].isin(candidates)].drop('text', axis=1)\n","  candidate_products['search_term'] = search\n","  reorder_cols = ['product_uid', 'product_title', 'search_term', 'combined_attr', 'brand', 'product_description']\n","  return candidate_products[reorder_cols]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lBTS6sERzPtf"},"source":["def main(srch, n):\n","  test_set = get_candidates(srch, 100)\n","  test_set['relevance'] = get_search_relevance(test_set)\n","  return test_set.sort_values('relevance', ascending=False).iloc[:n]['product_title']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5uDXuZ1vad4X"},"source":["## Final Testing"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pLnczQDt4ieY","executionInfo":{"status":"ok","timestamp":1624201881112,"user_tz":-330,"elapsed":5684,"user":{"displayName":"Kriz Moses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5AHG3pEQIY30An0gBgkynBoQrUDYQ22HSAT0p7A=s64","userId":"11504316763430471705"}},"outputId":"553e8c26-77ee-499c-91cc-ada3e0ee3ce2"},"source":["import time \n","start = time.time()\n","\n","search = 'pan'\n","print('The most relevant products to the search \"{}\" are:'.format(search))\n","for i, prod in enumerate(main(search, 10)):\n","  print(i+1, prod)\n","\n","end = time.time()\n","print('Time taken:', round(end-start,3), 'seconds')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The most relevant products to the search \"pan\" are:\n","1 Ozeri 8 in., 10 in., 12 in. Green Earth Frying Pan Set with Textured Ceramic Non-Stick Coating (100% PTFE and PFOA Free)\n","2 Range Kleen 10 in. Preferred Nonstick Fry Pan in Stainless Steel\n","3 Rubbermaid 9.38 in. L x 10 in. W x 11.75 in. H Metal in Cabinet Stand Alone Pan Organizer\n","4 Lava Signature 10-1/2 in. x 18-1/2 in. Enameled Cast Iron Roasting-Baking Pan in Orange Spice\n","5 Lava ECO 10-1/2 in. x 15-1/2 in. Enameled Cast Iron Grill Pan in Slate Black\n","6 Lava Signature 10-1/2 in. x 18-1/2 in. Enameled Cast Iron Roasting-Baking Pan in Cayenne Red\n","7 Lava ECO 10-1/2 in. x 19-3/4 in. Enameled Cast Iron Reversible Grill and Griddle Pan in Slate Black\n","8 Estwing 20 oz. 14 in. Steel Gold Pan\n","9 Lava ECO 15 in. x 13-1/4 in. Enameled Cast Iron Round Grill Pan in Slate Black\n","10 Lava ECO 10-1/2 in. x 13-1/2 in. Enameled Cast Iron Square Grill Pan in Slate Black\n","Time taken: 5.14 seconds\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XzxmWQJv5A2n","executionInfo":{"status":"ok","timestamp":1624200646420,"user_tz":-330,"elapsed":5864,"user":{"displayName":"Kriz Moses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5AHG3pEQIY30An0gBgkynBoQrUDYQ22HSAT0p7A=s64","userId":"11504316763430471705"}},"outputId":"22780717-bd75-45ff-9db1-bb8f841fc9f7"},"source":["search = 'air conditioner'\n","print('The most relevant products to the search \"{}\" are:'.format(search))\n","for i, prod in enumerate(main(search, 10)):\n","  print(i+1, prod)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The most relevant products to the search \"air conditioner\" are:\n","1 Danby 10,000 BTU Window Air Conditioner with Remote\n","2 Danby 15,000 BTU Window Air Conditioner with Remote\n","3 LG Electronics 7,000 BTU Window Air Conditioner with Cool, Heat and Remote\n","4 LG Electronics 7,000 BTU Window Air Conditioner with Cool, Heat and Remote\n","5 Danby 12,000 BTU Window Air Conditioner with Remote\n","6 Whynter 14,000 BTU Portable Air Conditioner with Dehumidifer and Remote\n","7 LG Electronics 12,000 BTU Window Air Conditioner with Cool, Heat and Remote\n","8 Honeywell 14,000 BTU Portable Air Conditioner with Remote Control in Black and Silver\n","9 LG Electronics 23,500 BTU Window Air Conditioner with Cool, Heat and Remote\n","10 LG Electronics 7,500 BTU 115-Volt Window Air Conditioner with Cool, Heat and Remote\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aEgfRnrS7_Br","executionInfo":{"status":"ok","timestamp":1624200679195,"user_tz":-330,"elapsed":6068,"user":{"displayName":"Kriz Moses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5AHG3pEQIY30An0gBgkynBoQrUDYQ22HSAT0p7A=s64","userId":"11504316763430471705"}},"outputId":"13726285-72d6-4b15-aff9-4811efb0a06a"},"source":["search = 'aor condiotioner'\n","print('The most relevant products to the search \"{}\" are:'.format(search))\n","for i, prod in enumerate(main(search, 10)):\n","  print(i+1, prod)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The most relevant products to the search \"aor condiotioner\" are:\n","1 Danby 10,000 BTU Window Air Conditioner with Remote\n","2 Danby 15,000 BTU Window Air Conditioner with Remote\n","3 LG Electronics 7,000 BTU Window Air Conditioner with Cool, Heat and Remote\n","4 LG Electronics 7,000 BTU Window Air Conditioner with Cool, Heat and Remote\n","5 Danby 12,000 BTU Window Air Conditioner with Remote\n","6 Whynter 14,000 BTU Portable Air Conditioner with Dehumidifer and Remote\n","7 LG Electronics 12,000 BTU Window Air Conditioner with Cool, Heat and Remote\n","8 Honeywell 14,000 BTU Portable Air Conditioner with Remote Control in Black and Silver\n","9 LG Electronics 23,500 BTU Window Air Conditioner with Cool, Heat and Remote\n","10 LG Electronics 7,500 BTU 115-Volt Window Air Conditioner with Cool, Heat and Remote\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PjrxNlcJ8SNx","executionInfo":{"status":"ok","timestamp":1624200802177,"user_tz":-330,"elapsed":5814,"user":{"displayName":"Kriz Moses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5AHG3pEQIY30An0gBgkynBoQrUDYQ22HSAT0p7A=s64","userId":"11504316763430471705"}},"outputId":"dfd0b959-c8b9-4d33-b8be-2ea95ccab678"},"source":["search = 'warm water'\n","print('The most relevant products to the search \"{}\" are:'.format(search))\n","for i, prod in enumerate(main(search, 10)):\n","  print(i+1, prod)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The most relevant products to the search \"warm water\" are:\n","1 EcoSmart Energy Smart 200-Light LED Warm White Icicle Light Set\n","2 AquaPower 120-Volt 3 kW 0.46 GPM Compact Point-of-Use Electric Tankless Water Heater\n","3 AquaPower 240-Volt 5.7 kW 0.87 GPM Compact Point-of-Use Electric Tankless Water Heater\n","4 AquaPower 120-Volt 1.8 kW 0.27 GPM Compact Point-of-Use Electric Tankless Water Heater\n","5 60-Light Outdoor Warm White Decorative LED String Light\n","6 Zojirushi 4-Liter Micom Water Boiler and Warmer\n","7 Zojirushi 3-Liter Micom Water Boiler and Warmer\n","8 Pure Guardian 1.5 gal. 100-Hour Warm and Cool Mist Ultrasonic Humidifier\n","9 Crane 1 Gal. Warm Mist Humidifier - Aqua\n","10 Honeywell 1 Gal. Warm Moisture Filter Free Humidifier\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yeF8Uokh8W3V","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624201081630,"user_tz":-330,"elapsed":5700,"user":{"displayName":"Kriz Moses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5AHG3pEQIY30An0gBgkynBoQrUDYQ22HSAT0p7A=s64","userId":"11504316763430471705"}},"outputId":"987b6c70-bec3-4d02-de48-ab1dda1680c3"},"source":["search = 'cut hair'\n","print('The most relevant products to the search \"{}\" are:'.format(search))\n","for i, prod in enumerate(main(search, 10)):\n","  print(i+1, prod)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The most relevant products to the search \"cut hair\" are:\n","1 Remington Power Beard Hair Trimmer\n","2 Andis 1600-Watt Hang-Up Hair Dryer with Light\n","3 Fun World Grave Breaker with Hair and Lite-Up Eyes\n","4 Original FlexiSnake Hair Clog Tool\n","5 DANCO Hair Catcher for Shower in White\n","6 Andis 1600-Watt Hang-Up Ionic Hair Dryer with Light\n","7 DANCO Hair Catcher for Shower Drain in Chrome\n","8 interDesign Classico Over Cabinet Hair Care Organizer in Chrome\n","9 Andis 1600-Watt Hang-Up Ionic Hair Dryer with Light\n","10 Instant Power 67.6 oz. Hair and Grease Drain Opener\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kOhXksdG7ZpD","executionInfo":{"status":"ok","timestamp":1624201095882,"user_tz":-330,"elapsed":5587,"user":{"displayName":"Kriz Moses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5AHG3pEQIY30An0gBgkynBoQrUDYQ22HSAT0p7A=s64","userId":"11504316763430471705"}},"outputId":"29f17a82-31db-4ea1-86db-1922b4a545c5"},"source":["search = 'orange'\n","print('The most relevant products to the search \"{}\" are:'.format(search))\n","for i, prod in enumerate(main(search, 10)):\n","  print(i+1, prod)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The most relevant products to the search \"orange\" are:\n","1 Orange GLO 32 oz. 4-in-1 Hardwood Floor Cleaner and Polish\n","2 Orange 3-Tier Utility Cart\n","3 Orange GLO 32 oz. Hardwood Floor Cleaner\n","4 Stanley Panel Carry - Orange\n","5 Brite Star LED Orange Battery Operated Pumpkin Lights (Set of 10)\n","6 The Hillman Group 48 in. Reflective Rod Orange\n","7 Orange GLO 32 oz. Wood Furniture Cleaner and Polish\n","8 ArcMate 36 in. Industrial Orange Pick Up Reacher Tool\n","9 Everbilt 16 in. Orange Ground Stake\n","10 Everbilt 11 in. Orange Ground Stake\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xOgRYs4c7dI4","executionInfo":{"status":"ok","timestamp":1624201119210,"user_tz":-330,"elapsed":5670,"user":{"displayName":"Kriz Moses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5AHG3pEQIY30An0gBgkynBoQrUDYQ22HSAT0p7A=s64","userId":"11504316763430471705"}},"outputId":"12d7aef7-6d02-4da8-844f-beea241d51fa"},"source":["search = 'foot'\n","print('The most relevant products to the search \"{}\" are:'.format(search))\n","for i, prod in enumerate(main(search, 10)):\n","  print(i+1, prod)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The most relevant products to the search \"foot\" are:\n","1 Johnson 13 ft. Aluminum Grade Rod\n","2 Backyard Discovery Installed 10 ft. x 10 ft. Cedar Pergola\n","3 Backyard Discovery 10 ft. x 12 ft. Cedar Pergola\n","4 Lufkin 6 ft. Folding Wood Rule\n","5 King Canopy 12 ft. W x 20 ft. D Steel Expandable Canopy\n","6 Arrow Sheridan 10 ft. x 8 ft. Steel Storage Shed\n","7 Ceilingmax 100 sq. ft. Ceiling Grid Kit White\n","8 Sigman 10 ft. x 20 ft. Silver Heavy Duty Tarp\n","9 Pacific Casual Hampton Bay 10 ft. x 10 ft. Pitched Roof Line Portable Patio Gazebo Replacement Canopy\n","10 PRO-SERIES 12 ft. x 7 ft. x 5 ft. 2-Story Commercial Grade Rolling Scaffold Tower 1,500 lb. Load Capacity\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ASCfgoCw7i0r","executionInfo":{"status":"ok","timestamp":1624201146970,"user_tz":-330,"elapsed":5456,"user":{"displayName":"Kriz Moses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5AHG3pEQIY30An0gBgkynBoQrUDYQ22HSAT0p7A=s64","userId":"11504316763430471705"}},"outputId":"f6f3908e-737b-4f47-b557-4f0f152e2eaf"},"source":["search = '10'\n","print('The most relevant products to the search \"{}\" are:'.format(search))\n","for i, prod in enumerate(main(search, 10)):\n","  print(i+1, prod)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The most relevant products to the search \"10\" are:\n","1 10 ft. x 10 ft. 3-Tier Gazebo\n","2 Lumabase Tan/Kraft Electric Luminaria Kit (Set of 10)\n","3 interDesign Una 10 in. Waste Can in Black\n","4 interDesign Una 10 in. Waste Can in Slate Gray\n","5 Lumabase Pathway Clear String Lights (Set of 10)\n","6 10 ft. x 10 ft. Vinyl-Coated Steel Carport\n","7 Razor-Back 10 in. x 10 in. Steel Tamper\n","8 TruAire 10 in. x 10 in. 3-Way Wall/Ceiling Register\n","9 TruAire 10 in. x 10 in. White Return Air Grille\n","10 10 in. x 10 in. x 8 in. Concrete Deck Block\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OyrMHgjZV41S"},"source":["Run Time"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VDRh7dMAWyRJ","executionInfo":{"status":"ok","timestamp":1624275914618,"user_tz":-330,"elapsed":579,"user":{"displayName":"Kriz Moses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5AHG3pEQIY30An0gBgkynBoQrUDYQ22HSAT0p7A=s64","userId":"11504316763430471705"}},"outputId":"33f4eba2-4d52-4658-b6d7-94548a937221"},"source":["import time \n","search = 'pan'\n","\n","time_taken = []\n","\n","for i in range(100):\n","  start = time.time()\n","  main(search, 10)\n","  end = time.time()\n","  time_taken.append(end-start)\n","  \n","print('Average Run Time: {}'.format(np.mean(time_taken)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Average Run Time: 5.092467794418335\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xqkWzTMiY4lZ","executionInfo":{"status":"ok","timestamp":1624276822826,"user_tz":-330,"elapsed":56424,"user":{"displayName":"Kriz Moses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5AHG3pEQIY30An0gBgkynBoQrUDYQ22HSAT0p7A=s64","userId":"11504316763430471705"}},"outputId":"e4d31bbb-f661-45b3-8452-2545a9ec80cd"},"source":["import time \n","search = 'air conditioner black whirpool'\n","\n","time_taken = []\n","\n","for i in range(10):\n","  start = time.time()\n","  main(search, 10)\n","  end = time.time()\n","  time_taken.append(end-start)\n","  \n","print('Average Run Time: {}'.format(np.mean(time_taken)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Average Run Time: 5.598297357559204\n"],"name":"stdout"}]}]}