{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Final_short.ipynb","provenance":[],"collapsed_sections":["5uDXuZ1vad4X"],"toc_visible":true,"machine_shape":"hm","mount_file_id":"1WT0orNG1YrKYUin-btpO49vDonEHbVj7","authorship_tag":"ABX9TyPxoHq+axMLeZPCtmcamPHD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9yZqldcqTlZ5","executionInfo":{"status":"ok","timestamp":1623236764043,"user_tz":-330,"elapsed":577,"user":{"displayName":"Kriz Moses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjKBNCwUDTthTPvGUZCLjw9vO9n5ddvIshwxW6L=s64","userId":"11504316763430471705"}},"outputId":"753cf6fd-7f70-4dc6-fffc-36c9fe2d3227"},"source":["%cd /content/drive/MyDrive/Home_Depot_Case_Study/Workspace3\n","!pwd"],"execution_count":1,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/Home_Depot_Case_Study/Workspace3\n","/content/drive/MyDrive/Home_Depot_Case_Study/Workspace3\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kjFiYUkpUJcH"},"source":["### Input Instructions\n","\n","* The input needs to be in the form of a dataframe with four fields - id, product_uid, product_title, search_term. \n","* The input needs to be stored in 'test_set'"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":220},"id":"JbUmJDsU80pF","executionInfo":{"status":"ok","timestamp":1623236931279,"user_tz":-330,"elapsed":946,"user":{"displayName":"Kriz Moses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjKBNCwUDTthTPvGUZCLjw9vO9n5ddvIshwxW6L=s64","userId":"11504316763430471705"}},"outputId":"b0bd7563-df26-4dac-f122-de98e3e486d0"},"source":["train_df = pd.read_csv('train.csv', encoding='iso-8859-1')\n","tmp = pd.read_pickle('preprocessing/cleaned_df_test.pkl')\n","test_set = train_df.loc[tmp.index[:]].drop('relevance', axis=1)\n","test_set_true_relevance = train_df.loc[tmp.index[:]]['relevance']\n","print(test_set.shape)\n","test_set.head()"],"execution_count":26,"outputs":[{"output_type":"stream","text":["(14814, 4)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>product_uid</th>\n","      <th>product_title</th>\n","      <th>search_term</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>70761</th>\n","      <td>211945</td>\n","      <td>198583</td>\n","      <td>WEN 32 in. Bench Grinder Pedestal Stand with W...</td>\n","      <td>crock pot water spigot</td>\n","    </tr>\n","    <tr>\n","      <th>65893</th>\n","      <td>198123</td>\n","      <td>187563</td>\n","      <td>Libman Wood Floor Sponge Mop</td>\n","      <td>can you use sponge mop</td>\n","    </tr>\n","    <tr>\n","      <th>18905</th>\n","      <td>58391</td>\n","      <td>115538</td>\n","      <td>Defiant 110รฅยก White Motion Sensing Outdoor Sec...</td>\n","      <td>honeywell motion sensor outdoor lights</td>\n","    </tr>\n","    <tr>\n","      <th>32031</th>\n","      <td>97935</td>\n","      <td>130548</td>\n","      <td>BLACK+DECKER 36-Volt Lithium-Ion Battery</td>\n","      <td>black and decker 36v</td>\n","    </tr>\n","    <tr>\n","      <th>67726</th>\n","      <td>203399</td>\n","      <td>191672</td>\n","      <td>Klein Tools 6-Piece Trim-Out Set</td>\n","      <td>veneer trim tool</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["           id  ...                             search_term\n","70761  211945  ...                  crock pot water spigot\n","65893  198123  ...                  can you use sponge mop\n","18905   58391  ...  honeywell motion sensor outdoor lights\n","32031   97935  ...                    black and decker 36v\n","67726  203399  ...                        veneer trim tool\n","\n","[5 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"markdown","metadata":{"id":"yDLarWmTbLPI"},"source":["### Imports and Loadings"]},{"cell_type":"code","metadata":{"id":"PC90DMJ0amjN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623236788812,"user_tz":-330,"elapsed":5671,"user":{"displayName":"Kriz Moses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjKBNCwUDTthTPvGUZCLjw9vO9n5ddvIshwxW6L=s64","userId":"11504316763430471705"}},"outputId":"421d2ae9-8773-410d-a5d5-785aba0e3fe9"},"source":["import pandas as pd \n","import numpy as np \n","import regex as re\n","!pip install nltk \n","import nltk\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","from collections import Counter\n","from nltk.stem import PorterStemmer \n","from wordcloud import STOPWORDS \n","from prettytable import PrettyTable\n","import warnings\n","warnings.filterwarnings('ignore')\n","import math\n","from nltk import sent_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from gensim.models import Word2Vec\n","from numpy.linalg import norm\n","import pickle\n","from tqdm.notebook import tqdm\n","from scipy.stats import uniform, randint, loguniform"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OuUhNdJMdeV9"},"source":["**CLEANING**"]},{"cell_type":"code","metadata":{"id":"hUwoh6xMbRA7","executionInfo":{"status":"ok","timestamp":1623236795093,"user_tz":-330,"elapsed":6291,"user":{"displayName":"Kriz Moses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjKBNCwUDTthTPvGUZCLjw9vO9n5ddvIshwxW6L=s64","userId":"11504316763430471705"}}},"source":["attr_df = pd.read_csv(\"attributes.csv\", encoding='iso-8859-1')\n","desc_df = pd.read_csv('product_descriptions.csv', encoding='iso-8859-1')\n","\n","def merge_attributes(df):\n","  product_uids = df['product_uid'].values\n","  temp = attr_df.loc[attr_df['product_uid'].isin(product_uids)].fillna('')  \n","  temp['name_value'] = temp['name'] + ' ' + temp['value']\n","  temp['combined_attr'] = temp.groupby(['product_uid'])['name_value'].transform(lambda x: ' '.join(x))\n","  temp = temp.drop_duplicates('product_uid')[['product_uid', 'combined_attr']]\n","  df = pd.merge(df, temp, on='product_uid', how='left').set_index(df.index)\n","  return df\n","\n","def merge_brand(df):\n","  product_uids = df['product_uid'].values\n","  temp = attr_df.loc[attr_df['product_uid'].isin(product_uids)]  \n","  brands = temp[temp['name']=='MFG Brand Name']\n","  brands_temp = brands[['product_uid','value']]\n","  df = pd.merge(df, brands_temp, on='product_uid', how='left').set_index(df.index)\n","  df.rename(columns = {'value':'brand'}, inplace = True) \n","  return df\n","\n","def merge_description(df):\n","  df = pd.merge(df, desc_df, on='product_uid', how='left').set_index(df.index)\n","  #an extra preprocessing step is performed to seperate the concatenated words in the description. \n","  df['product_description'] = df['product_description'].apply(lambda x: ' '.join(re.findall(r'[A-Z]?[^A-Z\\s]+|[A-Z]+', x)))\n","  return df"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"2obCBgaEbQ5Z","executionInfo":{"status":"ok","timestamp":1623237024504,"user_tz":-330,"elapsed":546,"user":{"displayName":"Kriz Moses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjKBNCwUDTthTPvGUZCLjw9vO9n5ddvIshwxW6L=s64","userId":"11504316763430471705"}}},"source":["with open('Final/cleaning/unique_brands.pkl','rb') as f:\n","  unique_brands = pickle.load(f)\n","\n","def first_n(n, sent):\n","  if n > len(sent.split()):\n","    return 'error101'\n","  return ' '.join(sent.split()[:n])\n","\n","def fillna_brand(data, unique_brnds):\n","  null_df = data[data['brand'].isnull()]\n","  notnull_df = data.dropna()\n","\n","  for i, row in null_df.iterrows():\n","    title = row['product_title']\n","    if first_n(4, title) in unique_brnds:\n","      null_df['brand'].loc[i] = first_n(4, title)\n","    elif first_n(3, title) in unique_brnds:\n","      null_df['brand'].loc[i] = first_n(3, title)\n","    elif first_n(2, title) in unique_brnds:\n","      null_df['brand'].loc[i] = first_n(2, title)\n","    else:\n","      null_df['brand'].loc[i] = first_n(1, title)\n","    \n","  data['brand'].loc[null_df.index] = null_df['brand'].values\n","  return data\n","\n","def fillna_attributes(data):\n","  null_df = data[data['combined_attr'].isnull()]\n","  null_df['combined_attr'] = null_df['product_description'].copy()\n","  data['combined_attr'].loc[null_df.index] = null_df['combined_attr'].values\n","  return data"],"execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"id":"_rO19Y1hb8kP","executionInfo":{"status":"ok","timestamp":1623236795942,"user_tz":-330,"elapsed":3,"user":{"displayName":"Kriz Moses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjKBNCwUDTthTPvGUZCLjw9vO9n5ddvIshwxW6L=s64","userId":"11504316763430471705"}}},"source":["def standardize_units(text):\n","  text = \" \" + text + \" \"\n","  text = re.sub('( gal | gals | galon )',' gallon ',text)\n","  text = re.sub('( ft | fts | feets | foot | foots )',' feet ',text)\n","  text = re.sub('( squares | sq )',' square ',text)\n","  text = re.sub('( lb | lbs | pounds )',' pound ',text)\n","  text = re.sub('( oz | ozs | ounces | ounc )',' ounce ',text)\n","  text = re.sub('( yds | yd | yards )',' yard ',text)\n","  return text\n","\n","def preprocessing(sent):\n","  sent = sent.replace('in.', ' inch ') #If we dont to this then 'in.' will be turned to 'in' in the next step\n","  words = re.split(r'\\W+', sent)\n","  words = [word.lower() for word in words]\n","  res = re.sub(\"[A-Za-z]+\", lambda ele: \" \" + ele[0] + \" \", ' '.join(words)) #add space between number and alphabets in a string\n","  cleaned = standardize_units(res) \n","  cleaned = ' '.join(cleaned.split()) #removing extra whitespaces\n","  return cleaned\n","\n","def preprocessing_search(sent):\n","  sent = sent.replace('in.', ' inch ')\n","  words = re.split(r'\\W+', sent)\n","  words = [word.lower() for word in words]\n","  res = re.sub(\"[A-Za-z]+\", lambda ele: \" \" + ele[0] + \" \", ' '.join(words)) #add space between number and alphabets in a string\n","  res = standardize_units(res) \n","  res = res.replace(' in ', ' inch ') #in search_terms 'in' is used more for 'inch' than as a preposition hence this step shouldn't hurt\n","  cleaned = ' '.join(res.split()) #removing extra whitespaces\n","  return cleaned"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"ORvlQx9hbQY0","executionInfo":{"status":"ok","timestamp":1623236808842,"user_tz":-330,"elapsed":12902,"user":{"displayName":"Kriz Moses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjKBNCwUDTthTPvGUZCLjw9vO9n5ddvIshwxW6L=s64","userId":"11504316763430471705"}}},"source":["def words(text): return re.findall(r'\\w+', text.lower())\n","WORDS = Counter(words(open('Final/cleaning/corpus.txt').read()))\n","\n","def P(word, N=sum(WORDS.values())): \n","    \"Probability of `word`.\"\n","    return WORDS[word] / N\n","def correction(word): \n","    \"Most probable spelling correction for word.\"\n","    return max(candidates(word), key=P)\n","def candidates(word): \n","    \"Generate possible spelling corrections for word.\"\n","    return (known([word]) or known(edits1(word)) or known(edits2(word)) or set([word]))\n","def known(words): \n","    \"The subset of `words` that appear in the dictionary of WORDS.\"\n","    return set(w for w in words if w in WORDS)\n","def edits1(word):\n","    \"All edits that are one edit away from `word`.\"\n","    letters    = 'abcdefghijklmnopqrstuvwxyz'\n","    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n","    deletes    = [L + R[1:]               for L, R in splits if R]\n","    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n","    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n","    inserts    = [L + c + R               for L, R in splits for c in letters]\n","    return set(deletes + transposes + replaces + inserts)\n","def edits2(word): \n","    \"All edits that are two edits away from `word`.\"\n","    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n","def corrected_term(term):\n","  temp = term.lower().split()\n","  temp = [correction(word) for word in temp]\n","  return ' '.join(temp)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"yZtraXWmcc9t","executionInfo":{"status":"ok","timestamp":1623236808845,"user_tz":-330,"elapsed":31,"user":{"displayName":"Kriz Moses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjKBNCwUDTthTPvGUZCLjw9vO9n5ddvIshwxW6L=s64","userId":"11504316763430471705"}}},"source":["#stop word removal and stemming\n","#We didn't do this before because we wanted to fix the typos in the searh term first \n","porter = PorterStemmer()\n","stp_wrds = set(stopwords.words('english'))\n","\n","def futher_preprocessing(sent):\n","  sent = sent.replace('_', ' _ ')\n","  words = sent.split()\n","  words = [w for w in words if not w in stp_wrds]\n","  words = [porter.stem(word) for word in words]\n","  return ' '.join(words)\n","\n","#stop word removal only - no stemming\n","def futher_preprocessing_without_stem(sent):\n","  sent = sent.replace('_', ' _ ')\n","  words = sent.split()\n","  words = [w for w in words if not w in stp_wrds]\n","  return ' '.join(words)"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zRANmdizdgys"},"source":["**FEATURIZATION**"]},{"cell_type":"code","metadata":{"id":"5nRw-5G3d6ll","executionInfo":{"status":"ok","timestamp":1623236808846,"user_tz":-330,"elapsed":28,"user":{"displayName":"Kriz Moses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjKBNCwUDTthTPvGUZCLjw9vO9n5ddvIshwxW6L=s64","userId":"11504316763430471705"}}},"source":["def common_words(df, col1, col2):\n","  \"\"\"\n","  Returns common words between each row of col1 and col2 of df in the form of a list. \n","  Length of list is number of rows in dataframe\n","  \"\"\"\n","  common_list = []\n","  for i, row in df[[col1,col2]].iterrows():\n","    set1 = set(row[col1].split())\n","    set2 = set(row[col2].split())\n","    common = set1 & set2\n","    common = ' '.join(common)\n","    common_list.append(common)\n","  return common_list\n","\n","def cosine_similarity_sent(sent1, sent2):\n","  \"\"\"\n","  Cosine Similarity between 2 sentences treating them as sets of words\n","  \"\"\"\n","  set1 = set(sent1.split())\n","  set2 = set(sent2.split())\n","  numerator = len(set1 & set2)\n","  denominator = math.sqrt(len(set1)) * math.sqrt(len(set2))\n","  if not denominator:\n","      return 0.0\n","  else:\n","      return numerator / denominator\n","\n","def jacquard_coefficient_sent(sent1, sent2):\n","  \"\"\"\n","  Jacquard Coefficient between 2 sentences treating them as sets of words\n","  \"\"\"\n","  set1 = set(sent1.split())\n","  set2 = set(sent2.split())\n","  numerator = len(set1 & set2)\n","  denominator = len(set1 | set2)\n","  if not denominator:\n","      return 0.0\n","  else:\n","      return numerator / denominator"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"UiEOJlltd4Rs","executionInfo":{"status":"ok","timestamp":1623236808847,"user_tz":-330,"elapsed":26,"user":{"displayName":"Kriz Moses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjKBNCwUDTthTPvGUZCLjw9vO9n5ddvIshwxW6L=s64","userId":"11504316763430471705"}}},"source":["corrected_feat_set = ['num_common_ST','num_common_SD', 'num_common_SB', 'cosine_ST', 'cosine_SD', \n","                'cosine_SB', 'jacquard_ST', 'jacquard_SD', 'jacquard_SB', \n","                'len_description', 'len_brand', 'len_title', 'len_search',\n","                'islast_ST', 'islast_SD', 'islast_SB']\n","\n","raw_feat_set = ['num_common_r_ST', 'num_common_r_SD', 'num_common_r_SB', 'cosine_r_ST', 'cosine_r_SD', \n","                'cosine_r_SB',  'jacquard_r_ST', 'jacquard_r_SD', 'jacquard_r_SB', \n","                'len_description', 'len_brand', 'len_title', 'len_r_search', \n","                'islast_r_ST','islast_r_SD', 'islast_r_SB']\n","\n","feat_set1_comb =  ['num_common_ST','num_common_SD', 'num_common_SB', 'cosine_ST', 'cosine_SD', \n","                'cosine_SB', 'jacquard_ST', 'jacquard_SD', 'jacquard_SB', \n","                'len_description', 'len_brand', 'len_title', 'len_search',\n","                'islast_ST', 'islast_SD', 'islast_SB', 'num_common_r_ST', 'num_common_r_SD', 'num_common_r_SB', 'cosine_r_ST', 'cosine_r_SD', \n","                'cosine_r_SB',  'jacquard_r_ST', 'jacquard_r_SD', 'jacquard_r_SB', \n","                'len_r_search', 'islast_r_ST','islast_r_SD', 'islast_r_SB']"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"fhj9Y65Je784","executionInfo":{"status":"ok","timestamp":1623236808847,"user_tz":-330,"elapsed":24,"user":{"displayName":"Kriz Moses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjKBNCwUDTthTPvGUZCLjw9vO9n5ddvIshwxW6L=s64","userId":"11504316763430471705"}}},"source":["def cosine_similarity_vec(a, b):\n","  \"\"\"\n","  Cosine Similarity between 2 vectors\n","  \"\"\"  \n","  num = np.dot(a, b)\n","  den = norm(a)*norm(b)\n","  if den != 0:\n","    return num/den\n","  else:\n","    return 0\n","\n","def jacquard_similarity_vec(a, b):\n","  \"\"\"\n","  Cosine Similarity between 2 vectors\n","  \"\"\"  \n","  num = np.dot(a,b)\n","  den = norm(a)**2 + norm(b)**2 - np.dot(a,b)\n","  if den != 0:\n","    return num/den\n","  else:\n","    return 0\n","\n","def inner_product_vec(a, b):\n","  return np.dot(a,b)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"y-AcPKuai-3G","executionInfo":{"status":"ok","timestamp":1623236824034,"user_tz":-330,"elapsed":15209,"user":{"displayName":"Kriz Moses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjKBNCwUDTthTPvGUZCLjw9vO9n5ddvIshwxW6L=s64","userId":"11504316763430471705"}}},"source":["with open('Final/featurization/F2_tfidf_search.pkl','rb') as f:\n","  F2_tfidf_search = pickle.load(f)\n","with open('Final/featurization/F2_tsvd_search.pkl','rb') as f:\n","  F2_tsvd_search = pickle.load(f)\n","\n","with open('Final/featurization/F2_tfidf_title.pkl','rb') as f:\n","  F2_tfidf_title = pickle.load(f)\n","with open('Final/featurization/F2_tsvd_title.pkl','rb') as f:\n","  F2_tsvd_title = pickle.load(f)\n","\n","with open('Final/featurization/F2_tfidf_desc.pkl','rb') as f:\n","  F2_tfidf_desc = pickle.load(f)\n","with open('Final/featurization/F2_tsvd_desc.pkl','rb') as f:\n","  F2_tsvd_desc = pickle.load(f)\n","\n","with open('Final/featurization/F2_tfidf_lsi.pkl','rb') as f:\n","  F2_tfidf_lsi = pickle.load(f)\n","with open('Final/featurization/F2_tsvd_lsi.pkl','rb') as f:\n","  F2_tsvd_lsi = pickle.load(f)"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"vXXBxqH_kbPq","executionInfo":{"status":"ok","timestamp":1623236824918,"user_tz":-330,"elapsed":888,"user":{"displayName":"Kriz Moses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjKBNCwUDTthTPvGUZCLjw9vO9n5ddvIshwxW6L=s64","userId":"11504316763430471705"}}},"source":["def lmir_fit(corpus):\n","  words = ' '.join(corpus).split()\n","  freq_dict = Counter(words)\n","  total_words = len(words)\n","  params = {\n","      'freq_dict':freq_dict,\n","      'total_words':total_words\n","  }\n","  return params\n","\n","def lmir_jm_score(query, doc, params, lambd):\n","  query = query.split()\n","  doc = doc.split()\n","  if len(doc) != 0 and len(query) != 0:\n","    eps = 0.0001/(params['total_words'])\n","    score = 0\n","    for word in query:\n","      p_ml = doc.count(word) / len(doc)\n","      if word in params['freq_dict'].keys():\n","        p_c = params['freq_dict'][word] / params['total_words']\n","      else: \n","        p_c = 0\n","      score += np.log(lambd*p_ml + (1-lambd)*p_c + eps)\n","    return score\n","\n","def lmir_dir_score(query, doc, params, mu):\n","  query = query.split()\n","  doc = doc.split()\n","  if len(doc) != 0 and len(query) != 0:\n","    eps = 0.0001/(params['total_words'])\n","    score = 0\n","    for word in query:\n","      p_ml = doc.count(word) / len(doc)\n","      if word in params['freq_dict']:\n","        p_c = params['freq_dict'][word] / params['total_words']\n","      else: \n","        p_c = 0\n","      lambd = len(doc) / (len(doc) + mu)\n","      score += np.log(lambd*p_ml + (1-lambd)*p_c + eps)\n","    return score\n","\n","def lmir_abs_score(query, doc, alpha):\n","  query = query.split()\n","  doc = doc.split()\n","  if len(doc) != 0 and len(query) != 0:\n","    score = 0\n","    temp_dict = {k:v+alpha for k,v in Counter(doc).items()}\n","    for word in query:\n","      if word in temp_dict:\n","        pass\n","      else:\n","        temp_dict[word] = alpha\n","    denominator = sum(temp_dict.values())\n","    for word in query:\n","      score += temp_dict[word] / denominator\n","  return score\n","\n","with open('Final/featurization/F3_LM_params_title.pkl','rb') as f:\n","  params_title_LM = pickle.load(f)\n","with open('Final/featurization/F3_LM_params_brand.pkl','rb') as f:\n","  params_brand_LM = pickle.load(f)\n","with open('Final/featurization/F3_LM_params_desc.pkl','rb') as f:\n","  params_desc_LM = pickle.load(f)"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"zBn9p-G9lMd2","executionInfo":{"status":"ok","timestamp":1623236827179,"user_tz":-330,"elapsed":2263,"user":{"displayName":"Kriz Moses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjKBNCwUDTthTPvGUZCLjw9vO9n5ddvIshwxW6L=s64","userId":"11504316763430471705"}}},"source":["def bm25_fit(corpus):\n","  tfidf_model = TfidfVectorizer(smooth_idf=False, token_pattern=r\"(?u)\\b\\w+\\b\")\n","  tfidf_model.fit(corpus)\n","  idf_dict = dict(zip(tfidf_model.get_feature_names(), list(tfidf_model.idf_)))\n","  avgdl = np.mean([len(doc.split()) for doc in corpus])\n","  params = {'idf_dict': idf_dict, \n","            'avgdl' : avgdl,\n","            'N' : N}\n","  return params\n","\n","def bm25_score(query, doc, params, k=0.1, b=0.5):\n","  idf_dict = params['idf_dict']\n","  avgdl = params['avgdl']\n","  N = params['N']\n","  score_query = 0\n","  for word in query.split():\n","    dl = len(doc.split())\n","    tf = doc.count(word)\n","    if word in idf_dict.keys():\n","      idf = idf_dict[word]\n","    else: \n","      idf = np.log(N+1) \n","    score_word = idf*(tf*(k+1))/(tf + k*(1-b) + b*dl/avgdl)\n","    score_query += score_word\n","  return score_query\n","\n","with open('Final/featurization/F3_bm25_params_title.pkl','rb') as f:\n","  params_title_bm25 = pickle.load(f)\n","with open('Final/featurization/F3_bm25_params_desc.pkl','rb') as f:\n","  params_desc_bm25 = pickle.load(f)\n","with open('Final/featurization/F3_bm25_params_brand.pkl','rb') as f:\n","  params_brand_bm25 = pickle.load(f)"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"-DzJ3h-xl0nu","executionInfo":{"status":"ok","timestamp":1623236830988,"user_tz":-330,"elapsed":3811,"user":{"displayName":"Kriz Moses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjKBNCwUDTthTPvGUZCLjw9vO9n5ddvIshwxW6L=s64","userId":"11504316763430471705"}}},"source":["with open('glove_vectors', 'rb') as f:\n","    model = pickle.load(f)\n","    glove_words =  set(model.keys())\n","\n","with open('Final/featurization/F3_tfidf_w2v_params_search.pkl','rb') as f:\n","  tfidf_w2v_params_search = pickle.load(f)\n","with open('Final/featurization/F3_tfidf_w2v_params_title.pkl','rb') as f:\n","  tfidf_w2v_params_title = pickle.load(f)\n","with open('Final/featurization/F3_tfidf_w2v_params_desc.pkl','rb') as f:\n","  tfidf_w2v_params_desc = pickle.load(f)"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"sDGERC9Ln-44","executionInfo":{"status":"ok","timestamp":1623236831897,"user_tz":-330,"elapsed":918,"user":{"displayName":"Kriz Moses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjKBNCwUDTthTPvGUZCLjw9vO9n5ddvIshwxW6L=s64","userId":"11504316763430471705"}}},"source":["with open('Final/featurization/F3_SmM_params_title.pkl','rb') as f:\n","  F3_SmM_params_title = pickle.load(f)\n","with open('Final/featurization/F3_SmM_params_brand.pkl','rb') as f:\n","  F3_SmM_params_brand = pickle.load(f)\n","with open('Final/featurization/F3_SmM_params_desc.pkl','rb') as f:\n","  F3_SmM_params_desc = pickle.load(f)"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7GMIPMDEpMy-"},"source":["**MODELLING**"]},{"cell_type":"code","metadata":{"id":"JwSCXo2ppRSh","executionInfo":{"status":"ok","timestamp":1623236838806,"user_tz":-330,"elapsed":6912,"user":{"displayName":"Kriz Moses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjKBNCwUDTthTPvGUZCLjw9vO9n5ddvIshwxW6L=s64","userId":"11504316763430471705"}}},"source":["F1_scaler_ridge = pickle.load(open('Final/modelling/base_models/F1_scaler_ridge.pkl', 'rb'))\n","F1_scaler_lasso = pickle.load(open('Final/modelling/base_models/F1_scaler_lasso.pkl', 'rb'))\n","F1_scaler_en = pickle.load(open('Final/modelling/base_models/F1_scaler_en.pkl', 'rb'))\n","F1_scaler_svr = pickle.load(open('Final/modelling/base_models/F1_scaler_svr.pkl', 'rb'))\n","\n","F1_xgb = pickle.load(open('Final/modelling/base_models/F1_xgb.pkl', 'rb'))\n","F1_rf = pickle.load(open('Final/modelling/base_models/F1_rf.pkl', 'rb'))\n","F1_ridge = pickle.load(open('Final/modelling/base_models/F1_ridge.pkl', 'rb'))\n","F1_lasso = pickle.load(open('Final/modelling/base_models/F1_lasso.pkl', 'rb'))\n","F1_en = pickle.load(open('Final/modelling/base_models/F1_en.pkl', 'rb'))\n","F1_svr = pickle.load(open('Final/modelling/base_models/F1_svr.pkl', 'rb'))\n","F1_dt = pickle.load(open('Final/modelling/base_models/F1_dt.pkl', 'rb'))"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jp_TfVfbpm14","executionInfo":{"status":"ok","timestamp":1623236844191,"user_tz":-330,"elapsed":5395,"user":{"displayName":"Kriz Moses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjKBNCwUDTthTPvGUZCLjw9vO9n5ddvIshwxW6L=s64","userId":"11504316763430471705"}}},"source":["#Loading the standard scalers\n","F2_scaler_ridge = pickle.load(open('Final/modelling/base_models/F2_scaler_ridge.pkl', 'rb'))\n","F2_scaler_lasso = pickle.load(open('Final/modelling/base_models/F2_scaler_lasso.pkl', 'rb'))\n","F2_scaler_en = pickle.load(open('Final/modelling/base_models/F2_scaler_en.pkl', 'rb'))\n","F2_scaler_svr = pickle.load(open('Final/modelling/base_models/F2_scaler_svr.pkl', 'rb'))\n","\n","#Loading the models \n","F2_svr = pickle.load(open('Final/modelling/base_models/F2_svr.pkl', 'rb'))\n","F2_rf = pickle.load(open('Final/modelling/base_models/F2_rf.pkl', 'rb'))\n","F2_ridge = pickle.load(open('Final/modelling/base_models/F2_ridge.pkl', 'rb'))\n","F2_lasso = pickle.load(open('Final/modelling/base_models/F2_lasso.pkl', 'rb'))\n","F2_en = pickle.load(open('Final/modelling/base_models/F2_en.pkl', 'rb'))\n","F2_dt = pickle.load(open('Final/modelling/base_models/F2_dt.pkl', 'rb'))"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jwku4HRApsGa","executionInfo":{"status":"ok","timestamp":1623236847498,"user_tz":-330,"elapsed":3314,"user":{"displayName":"Kriz Moses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjKBNCwUDTthTPvGUZCLjw9vO9n5ddvIshwxW6L=s64","userId":"11504316763430471705"}}},"source":["#Loading the standard scalers\n","F3_scaler_en = pickle.load(open('Final/modelling/base_models/F3_scaler_en.pkl', 'rb'))\n","F3_scaler_ridge = pickle.load(open('Final/modelling/base_models/F3_scaler_ridge.pkl', 'rb'))\n","F3_scaler_lasso = pickle.load(open('Final/modelling/base_models/F3_scaler_lasso.pkl', 'rb'))\n","\n","#Loading the models \n","F3_ridge = pickle.load(open('Final/modelling/base_models/F3_ridge.pkl', 'rb'))\n","F3_lasso = pickle.load(open('Final/modelling/base_models/F3_lasso.pkl', 'rb'))\n","F3_en = pickle.load(open('Final/modelling/base_models/F3_en.pkl', 'rb'))\n","F3_dt = pickle.load(open('Final/modelling/base_models/F3_dt.pkl', 'rb'))"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"J1QNKmyOqCL4","executionInfo":{"status":"ok","timestamp":1623236848222,"user_tz":-330,"elapsed":734,"user":{"displayName":"Kriz Moses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjKBNCwUDTthTPvGUZCLjw9vO9n5ddvIshwxW6L=s64","userId":"11504316763430471705"}}},"source":["final_scaler = pickle.load(open('Final/modelling/meta_scaler.pkl', 'rb'))\n","final_ridge = pickle.load(open('Final/modelling/meta_ridge.pkl', 'rb'))"],"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5uDXuZ1vad4X"},"source":["### Final File"]},{"cell_type":"code","metadata":{"id":"HRkfSn71aggT","executionInfo":{"status":"ok","timestamp":1623236915057,"user_tz":-330,"elapsed":2741,"user":{"displayName":"Kriz Moses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjKBNCwUDTthTPvGUZCLjw9vO9n5ddvIshwxW6L=s64","userId":"11504316763430471705"}}},"source":["def get_search_relevance(test_set):\n","  #CLEANING\n","  test_set = merge_attributes(test_set)\n","  test_set = merge_brand(test_set)\n","  test_set = merge_description(test_set)\n","  test_set.drop('id',inplace=True, axis=1)\n","  test_set = fillna_brand(test_set, unique_brands)\n","  test_set = fillna_attributes(test_set)\n","  test_set = test_set.fillna('')\n","\n","  test_set['cleaned_title'] = test_set['product_title'].apply(lambda x : preprocessing(x))\n","  test_set['cleaned_brand'] = test_set['brand'].apply(lambda x : preprocessing(x))\n","  test_set['cleaned_description'] = test_set['product_description'].apply(lambda x : preprocessing(x))\n","  test_set['cleaned_attributes'] = test_set['combined_attr'].apply(lambda x : preprocessing(x))\n","  test_set['cleaned_search'] = test_set['search_term'].apply(lambda x : preprocessing_search(x))\n","\n","  test_set['corrected_search'] = test_set['cleaned_search'].apply(lambda x: corrected_term(x))\n","\n","  cleaned_test_set = pd.DataFrame() \n","  cleaned_test_set['title'] = test_set['cleaned_title'].apply(lambda x : futher_preprocessing(x)) \n","  cleaned_test_set['brand'] = test_set['cleaned_brand'].apply(lambda x : futher_preprocessing(x)) \n","  cleaned_test_set['description'] = test_set['cleaned_description'].apply(lambda x : futher_preprocessing(x)) \n","  cleaned_test_set['attributes'] = test_set['cleaned_attributes'].apply(lambda x : futher_preprocessing(x)) \n","  cleaned_test_set['search'] = test_set['cleaned_search'].apply(lambda x : futher_preprocessing(x)) \n","  cleaned_test_set['corrected_search'] = test_set['corrected_search'].apply(lambda x : futher_preprocessing(x)) \n","\n","  cleaned_test_set2 = pd.DataFrame()\n","  cleaned_test_set2['title'] = test_set['cleaned_title'].apply(lambda x : futher_preprocessing_without_stem(x)) \n","  cleaned_test_set2['brand'] = test_set['cleaned_brand'].apply(lambda x : futher_preprocessing_without_stem(x)) \n","  cleaned_test_set2['description'] = test_set['cleaned_description'].apply(lambda x : futher_preprocessing_without_stem(x))\n","  cleaned_test_set2['attributes'] = test_set['cleaned_attributes'].apply(lambda x : futher_preprocessing_without_stem(x)) \n","  cleaned_test_set2['search'] = test_set['cleaned_search'].apply(lambda x : futher_preprocessing_without_stem(x)) \n","  cleaned_test_set2['corrected_search'] = test_set['corrected_search'].apply(lambda x : futher_preprocessing_without_stem(x)) \n","\n","  cleaned_test_set['brand'] = cleaned_test_set['brand'].replace(to_replace =[\"\"], value =\"missing_brand\")\n","  cleaned_test_set2['brand'] = cleaned_test_set2['brand'].replace(to_replace =[\"\"], value =\"missing_brand\")\n","  cleaned_test_set['search'] = cleaned_test_set['search'].replace(to_replace =[\"\"], value =\"missing_search\")\n","  cleaned_test_set2['search'] = cleaned_test_set2['search'].replace(to_replace =[\"\"], value =\"missing_search\")\n","\n","  cleaned_test_set['attributes'] = cleaned_test_set['attributes'].apply(lambda x: re.sub('bullet \\d\\d ', '', x))\n","  cleaned_test_set2['attributes'] = cleaned_test_set2['attributes'].apply(lambda x: re.sub('bullet \\d\\d ', '', x))\n","\n","  cleaned_test_set['description'] = cleaned_test_set['description'].apply(lambda x: re.sub('bullet \\d\\d ', '', x))\n","  cleaned_test_set2['description'] = cleaned_test_set2['description'].apply(lambda x: re.sub('bullet \\d\\d ', '', x))\n","\n","  cleaned_test_set.rename(columns={\"search\": \"raw_search\"}, inplace=True)\n","  cleaned_test_set2.rename(columns={\"search\": \"raw_search\"}, inplace=True)\n","\n","  #FEATURIZATION\n","  #set1\n","  data1 = cleaned_test_set.copy()\n","\n","  data1['common_ST'] = common_words(data1,'corrected_search', 'title')\n","  data1['common_SD'] = common_words(data1,'corrected_search', 'description')\n","  data1['common_SB'] = common_words(data1,'corrected_search', 'brand')\n","  data1['common_r_ST'] = common_words(data1,'raw_search', 'title')\n","  data1['common_r_SD'] = common_words(data1,'raw_search', 'description')\n","  data1['common_r_SB'] = common_words(data1,'raw_search', 'brand')\n","\n","  data1['num_common_ST'] = data1['common_ST'].apply(lambda x : len(x.split()))\n","  data1['num_common_SD'] = data1['common_SD'].apply(lambda x : len(x.split()))\n","  data1['num_common_SB'] = data1['common_SB'].apply(lambda x : len(x.split()))\n","  data1['num_common_r_ST'] = data1['common_r_ST'].apply(lambda x : len(x.split()))\n","  data1['num_common_r_SD'] = data1['common_r_SD'].apply(lambda x : len(x.split()))\n","  data1['num_common_r_SB'] = data1['common_r_SB'].apply(lambda x : len(x.split()))\n","\n","  data1['cosine_ST'] = data1.apply(lambda row: cosine_similarity_sent(row['corrected_search'], row['title']), axis=1) \n","  data1['cosine_SD'] = data1.apply(lambda row: cosine_similarity_sent(row['corrected_search'], row['description']), axis=1)\n","  data1['cosine_SB'] = data1.apply(lambda row: cosine_similarity_sent(row['corrected_search'], row['brand']), axis=1)\n","  data1['cosine_r_ST'] = data1.apply(lambda row: cosine_similarity_sent(row['raw_search'], row['title']), axis=1) \n","  data1['cosine_r_SD'] = data1.apply(lambda row: cosine_similarity_sent(row['raw_search'], row['description']), axis=1)\n","  data1['cosine_r_SB'] = data1.apply(lambda row: cosine_similarity_sent(row['raw_search'], row['brand']), axis=1)\n","\n","  data1['jacquard_ST'] = data1.apply(lambda row: jacquard_coefficient_sent(row['corrected_search'], row['title']), axis=1) \n","  data1['jacquard_SD'] = data1.apply(lambda row: jacquard_coefficient_sent(row['corrected_search'], row['description']), axis=1)\n","  data1['jacquard_SB'] = data1.apply(lambda row: jacquard_coefficient_sent(row['corrected_search'], row['brand']), axis=1)\n","  data1['jacquard_r_ST'] = data1.apply(lambda row: jacquard_coefficient_sent(row['raw_search'], row['title']), axis=1) \n","  data1['jacquard_r_SD'] = data1.apply(lambda row: jacquard_coefficient_sent(row['raw_search'], row['description']), axis=1)\n","  data1['jacquard_r_SB'] = data1.apply(lambda row: jacquard_coefficient_sent(row['raw_search'], row['brand']), axis=1)\n","\n","  data1['len_description'] = data1['description'].apply(lambda x : len(x.split()))\n","  data1['len_brand'] = data1['brand'].apply(lambda x : len(x.split()))\n","  data1['len_title'] = data1['title'].apply(lambda x : len(x.split()))\n","  data1['len_search'] = data1['corrected_search'].apply(lambda x : len(x.split()))\n","  data1['len_r_search'] = data1['raw_search'].apply(lambda x : len(x.split()))\n","\n","  data1['islast_ST'] = data1.apply(lambda row: row['corrected_search'].split()[-1] in row['title'].split(), axis=1)\n","  data1['islast_SD'] = data1.apply(lambda row: row['corrected_search'].split()[-1] in row['description'].split(), axis=1)\n","  data1['islast_SB'] = data1.apply(lambda row: row['corrected_search'].split()[-1] in row['brand'].split(), axis=1)\n","  data1['islast_r_ST'] = data1.apply(lambda row: row['raw_search'].split()[-1] in row['title'].split(), axis=1)\n","  data1['islast_r_SD'] = data1.apply(lambda row: row['raw_search'].split()[-1] in row['description'].split(), axis=1)\n","  data1['islast_r_SB'] = data1.apply(lambda row: row['raw_search'].split()[-1] in row['brand'].split(), axis=1)\n","\n","  bool_cols = ['islast_ST', 'islast_SD', 'islast_SB', 'islast_r_ST', 'islast_r_SD', 'islast_r_SB']\n","  for col in bool_cols:\n","    data1[col] = data1[col].astype(int)\n","\n","  #set2\n","  data2 = cleaned_test_set.copy()\n","\n","  X_search = F2_tfidf_search.transform(data2['corrected_search'])\n","  truncated_search = F2_tsvd_search.transform(X_search)\n","  X_title = F2_tfidf_title.transform(data2['title'])\n","  truncated_title = F2_tsvd_title.transform(X_title)\n","  X_desc = F2_tfidf_desc.transform(data2['description'])\n","  truncated_desc = F2_tsvd_desc.transform(X_desc)\n","  trun_arr = np.hstack((truncated_search,truncated_title,truncated_desc))\n","  truncated_df = pd.DataFrame(trun_arr, index=cleaned_test_set.index)\n","\n","  title_desc = data2[\"title\"].astype(str) + ' ' + data2[\"description\"].astype(str)\n","  X_title_desc = F2_tfidf_lsi.transform(title_desc)\n","  truncated_title_desc = F2_tsvd_lsi.transform(X_title_desc)\n","  X_search_ = F2_tfidf_lsi.transform(data2['corrected_search'])\n","  transformed_search = F2_tsvd_lsi.transform(X_search_)\n","\n","  cos_sim = []\n","  for i in range(len(transformed_search)):\n","    cos_sim.append(cosine_similarity_vec(truncated_title_desc[i], transformed_search[i]))\n","  data2['lsi_cos_sim'] = cos_sim\n","  jaq_sim = []\n","  for i in range(len(transformed_search)):\n","    jaq_sim.append(jacquard_similarity_vec(truncated_title_desc[i], transformed_search[i]))\n","  data2['lsi_jaq_sim'] = jaq_sim\n","  inn_prod = []\n","  for i in range(len(transformed_search)):\n","    inn_prod.append(inner_product_vec(truncated_title_desc[i], transformed_search[i]))\n","  data2['lsi_inn_prod'] = inn_prod\n","\n","  data2 = data2[['lsi_cos_sim', 'lsi_jaq_sim',\t'lsi_inn_prod']]\n","\n","  #set3\n","  data3 = cleaned_test_set.copy()\n","\n","  data3['JM_ST'] = data3.apply(lambda row: lmir_jm_score(row['corrected_search'], row['title'], params_title_LM, 0.9), axis=1)\n","  data3['Dir_ST'] = data3.apply(lambda row: lmir_dir_score(row['corrected_search'], row['title'], params_title_LM, 12 ), axis=1)\n","  data3['AD_ST'] = data3.apply(lambda row: lmir_abs_score(row['corrected_search'], row['title'], 0.01 ), axis=1)\n","  data3['JM_SB'] = data3.apply(lambda row: lmir_jm_score(row['corrected_search'], row['brand'], params_brand_LM, 0.9), axis=1)\n","  data3['Dir_SB'] = data3.apply(lambda row: lmir_dir_score(row['corrected_search'], row['brand'], params_brand_LM, 1.5 ), axis=1)\n","  data3['AD_SB'] = data3.apply(lambda row: lmir_abs_score(row['corrected_search'], row['brand'], 0.01 ), axis=1)\n","  data3['JM_SD'] = data3.apply(lambda row: lmir_jm_score(row['corrected_search'], row['description'], params_desc_LM, 0.9), axis=1)\n","  data3['Dir_SD'] = data3.apply(lambda row: lmir_dir_score(row['corrected_search'], row['description'], params_desc_LM, 106 ), axis=1)\n","  data3['AD_SD'] = data3.apply(lambda row: lmir_abs_score(row['corrected_search'], row['description'], 0.01 ), axis=1)\n","\n","  data3['bm25_ST'] = data3.apply(lambda row: bm25_score(row['corrected_search'], row['title'], params_title_bm25 ), axis=1)\n","  data3['bm25_SD'] = data3.apply(lambda row: bm25_score(row['corrected_search'], row['description'], params_desc_bm25 ), axis=1)\n","  data3['bm25_SB'] = data3.apply(lambda row: bm25_score(row['corrected_search'], row['brand'], params_brand_bm25 ), axis=1)\n","\n","\n","  dictionary = tfidf_w2v_params_search['dictionary']\n","  tfidf_words = tfidf_w2v_params_search['tfidf_words']\n","  search_tfidf_w2v_test = []; # the avg-w2v for each sentence/review is stored in this list\n","  for sentence in cleaned_test_set2['corrected_search']: # for each review/sentence\n","      vector = np.zeros(300) # as word vectors are of zero length\n","      tf_idf_weight =0; # num of words with a valid vector in the sentence/review\n","      for word in sentence.split(): # for each word in a review/sentence\n","          if (word in glove_words) and (word in tfidf_words):\n","              vec = model[word] # getting the vector for each word\n","              # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)/len(sentence.split())))\n","              tf_idf = dictionary[word]*(sentence.count(word)) # getting the tfidf value for each word\n","              vector += (vec * tf_idf) # calculating tfidf weighted w2v\n","              tf_idf_weight += tf_idf\n","      if tf_idf_weight != 0:\n","          vector /= tf_idf_weight\n","      search_tfidf_w2v_test.append(vector)\n","\n","\n","\n","  dictionary = tfidf_w2v_params_title['dictionary']\n","  tfidf_words = tfidf_w2v_params_title['tfidf_words']\n","  title_tfidf_w2v_test = []; # the avg-w2v for each sentence/review is stored in this list\n","  for sentence in cleaned_test_set2['title']: # for each review/sentence\n","      vector = np.zeros(300) # as word vectors are of zero length\n","      tf_idf_weight =0; # num of words with a valid vector in the sentence/review\n","      for word in sentence.split(): # for each word in a review/sentence\n","          if (word in glove_words) and (word in tfidf_words):\n","              vec = model[word] # getting the vector for each word\n","              # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)/len(sentence.split())))\n","              tf_idf = dictionary[word]*(sentence.count(word)) # getting the tfidf value for each word\n","              vector += (vec * tf_idf) # calculating tfidf weighted w2v\n","              tf_idf_weight += tf_idf\n","      if tf_idf_weight != 0:\n","          vector /= tf_idf_weight\n","      title_tfidf_w2v_test.append(vector)\n","\n","\n","  dictionary = tfidf_w2v_params_desc['dictionary']\n","  tfidf_words = tfidf_w2v_params_desc['tfidf_words']\n","  desc_tfidf_w2v_test = []; # the avg-w2v for each sentence/review is stored in this list\n","  for sentence in cleaned_test_set2['description']: # for each review/sentence\n","      vector = np.zeros(300) # as word vectors are of zero length\n","      tf_idf_weight =0; # num of words with a valid vector in the sentence/review\n","      for word in sentence.split(): # for each word in a review/sentence\n","          if (word in glove_words) and (word in tfidf_words):\n","              vec = model[word] # getting the vector for each word\n","              # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)/len(sentence.split())))\n","              tf_idf = dictionary[word]*(sentence.count(word)) # getting the tfidf value for each word\n","              vector += (vec * tf_idf) # calculating tfidf weighted w2v\n","              tf_idf_weight += tf_idf\n","      if tf_idf_weight != 0:\n","          vector /= tf_idf_weight\n","      desc_tfidf_w2v_test.append(vector)\n","\n","  arr1 = np.array(search_tfidf_w2v_test)\n","  arr2 = np.array(title_tfidf_w2v_test)\n","  arr3 = np.array(desc_tfidf_w2v_test)\n","  tfidf_w2v_df = pd.DataFrame(np.hstack((arr1, arr2, arr3)), index=cleaned_test_set2.index)\n","\n","  idf_dict = F3_SmM_params_title['idf_dict']\n","  N = F3_SmM_params_title['N']\n","  max_tf = []\n","  max_idf = []\n","  max_tfidf = []\n","  min_tf = []\n","  min_idf = []\n","  min_tfidf = []\n","  sum_tf = []\n","  sum_idf = []\n","  sum_tfidf = []\n","  for ind, row in cleaned_test_set.iterrows():\n","    search = row['corrected_search']\n","    text = row['title']\n","    tf_vals = []\n","    idf_vals = []\n","    tfidf_vals = []\n","    for word in search.split():\n","      if word in idf_dict.keys():\n","        tf = text.count(word)\n","        idf = idf_dict[word]\n","      else:\n","        tf = text.count(word)\n","        idf = np.log(N+1)\n","      tf_vals.append(tf)\n","      idf_vals.append(idf)\n","      tfidf_vals.append(tf*idf)\n","    max_tf.append(max(tf_vals))\n","    min_tf.append(min(tf_vals))\n","    sum_tf.append(sum(tf_vals))\n","    max_idf.append(max(idf_vals))\n","    min_idf.append(min(idf_vals))\n","    sum_idf.append(sum(idf_vals))\n","    max_tfidf.append(max(tfidf_vals))\n","    min_tfidf.append(min(tfidf_vals))\n","    sum_tfidf.append(sum(tfidf_vals))\n","\n","  data3['max_tf_ST'] = max_tf\n","  data3['max_idf_ST'] = max_idf\n","  data3['max_tfidf_ST'] = max_tfidf\n","  data3['min_tf_ST'] = min_tf\n","  data3['min_idf_ST'] = min_idf\n","  data3['min_tfidf_ST'] = min_tfidf\n","  data3['sum_tf_ST'] = sum_tf\n","  data3['sum_idf_ST'] = sum_idf\n","  data3['sum_tfidf_ST'] = sum_tfidf\n","\n","  idf_dict = F3_SmM_params_brand['idf_dict']\n","  N = F3_SmM_params_brand['N']\n","  max_tf = []\n","  max_idf = []\n","  max_tfidf = []\n","  min_tf = []\n","  min_idf = []\n","  min_tfidf = []\n","  sum_tf = []\n","  sum_idf = []\n","  sum_tfidf = []\n","  for ind, row in cleaned_test_set.iterrows():\n","    search = row['corrected_search']\n","    text = row['brand']\n","    tf_vals = []\n","    idf_vals = []\n","    tfidf_vals = []\n","    for word in search.split():\n","      if word in idf_dict.keys():\n","        tf = text.count(word)\n","        idf = idf_dict[word]\n","      else:\n","        tf = text.count(word)\n","        idf = np.log(N+1)\n","      tf_vals.append(tf)\n","      idf_vals.append(idf)\n","      tfidf_vals.append(tf*idf)\n","    max_tf.append(max(tf_vals))\n","    min_tf.append(min(tf_vals))\n","    sum_tf.append(sum(tf_vals))\n","    max_idf.append(max(idf_vals))\n","    min_idf.append(min(idf_vals))\n","    sum_idf.append(sum(idf_vals))\n","    max_tfidf.append(max(tfidf_vals))\n","    min_tfidf.append(min(tfidf_vals))\n","    sum_tfidf.append(sum(tfidf_vals))\n","\n","  data3['max_tf_SB'] = max_tf\n","  data3['max_idf_SB'] = max_idf\n","  data3['max_tfidf_SB'] = max_tfidf\n","  data3['min_tf_SB'] = min_tf\n","  data3['min_idf_SB'] = min_idf\n","  data3['min_tfidf_SB'] = min_tfidf\n","  data3['sum_tf_SB'] = sum_tf\n","  data3['sum_idf_SB'] = sum_idf\n","  data3['sum_tfidf_SB'] = sum_tfidf\n","\n","  idf_dict = F3_SmM_params_desc['idf_dict']\n","  N = F3_SmM_params_desc['N']\n","  max_tf = []\n","  max_idf = []\n","  max_tfidf = []\n","  min_tf = []\n","  min_idf = []\n","  min_tfidf = []\n","  sum_tf = []\n","  sum_idf = []\n","  sum_tfidf = []\n","  for ind, row in cleaned_test_set.iterrows():\n","    search = row['corrected_search']\n","    text = row['description']\n","    tf_vals = []\n","    idf_vals = []\n","    tfidf_vals = []\n","    for word in search.split():\n","      if word in idf_dict.keys():\n","        tf = text.count(word)\n","        idf = idf_dict[word]\n","      else:\n","        tf = text.count(word)\n","        idf = np.log(N+1)\n","      tf_vals.append(tf)\n","      idf_vals.append(idf)\n","      tfidf_vals.append(tf*idf)\n","    max_tf.append(max(tf_vals))\n","    min_tf.append(min(tf_vals))\n","    sum_tf.append(sum(tf_vals))\n","    max_idf.append(max(idf_vals))\n","    min_idf.append(min(idf_vals))\n","    sum_idf.append(sum(idf_vals))\n","    max_tfidf.append(max(tfidf_vals))\n","    min_tfidf.append(min(tfidf_vals))\n","    sum_tfidf.append(sum(tfidf_vals))\n","\n","  data3['max_tf_SD'] = max_tf\n","  data3['max_idf_SD'] = max_idf\n","  data3['max_tfidf_SD'] = max_tfidf\n","  data3['min_tf_SD'] = min_tf\n","  data3['min_idf_SD'] = min_idf\n","  data3['min_tfidf_SD'] = min_tfidf\n","  data3['sum_tf_SD'] = sum_tf\n","  data3['sum_idf_SD'] = sum_idf\n","  data3['sum_tfidf_SD'] = sum_tfidf\n","\n","  data3 = data3.iloc[:,6:]\n","\n","  #MODELLING\n","  X1 = pd.concat([data1[feat_set1_comb], data2, data3], axis=1)\n","  pred_xgb = F1_xgb.predict(X1) \n","  pred_rf = F1_rf.predict(X1) \n","  pred_ridge = F1_ridge.predict(F1_scaler_ridge.transform(X1)) \n","  pred_lasso = F1_lasso.predict(F1_scaler_lasso.transform(X1)) \n","  pred_en = F1_en.predict(F1_scaler_en.transform(X1)) \n","  pred_svr = F1_svr.predict(F1_scaler_svr.transform(X1)) \n","  pred_dt = F1_dt.predict(X1) \n","  arr = np.hstack((pred_xgb.reshape(-1,1),\n","                  pred_rf.reshape(-1,1), \n","                  pred_dt.reshape(-1,1), \n","                  pred_svr.reshape(-1,1),\n","                  pred_ridge.reshape(-1,1),\n","                  pred_lasso.reshape(-1,1), \n","                  pred_en.reshape(-1,1)))\n","  F1_df = pd.DataFrame(arr, columns=['f1_xgb', 'f1_rf', 'f1_dt', 'f1_svr', 'f1_ridge', 'f1_lasso', 'f1_en'], index=X1.index)\n","\n","  X2 = pd.concat([data1[feat_set1_comb], data2, data3, tfidf_w2v_df], axis=1)\n","  pred_svr = F2_svr.predict(F2_scaler_svr.transform(X2)) \n","  pred_rf = F2_rf.predict(X2) \n","  pred_ridge = F2_ridge.predict(F2_scaler_ridge.transform(X2)) \n","  pred_lasso = F2_lasso.predict(F2_scaler_lasso.transform(X2)) \n","  pred_en = F2_en.predict(F2_scaler_en.transform(X2)) \n","  pred_dt = F2_dt.predict(X2) \n","  arr = np.hstack((pred_svr.reshape(-1,1),\n","                  pred_rf.reshape(-1,1),                                    \n","                  pred_ridge.reshape(-1,1),\n","                  pred_lasso.reshape(-1,1), \n","                  pred_en.reshape(-1,1),\n","                  pred_dt.reshape(-1,1),))\n","  F2_df = pd.DataFrame(arr, columns=['f2_svr', 'f2_rf', 'f2_ridge', 'f2_lasso', 'f2_en', 'f2_dt'], index=X2.index)\n","\n","  X3 = pd.concat([data1[feat_set1_comb], data2, data3, truncated_df], axis=1)\n","  pred_dt = F3_dt.predict(X3) \n","  pred_ridge = F3_ridge.predict(F3_scaler_ridge.transform(X3)) \n","  pred_lasso = F3_lasso.predict(F3_scaler_lasso.transform(X3)) \n","  pred_en = F3_en.predict(F3_scaler_en.transform(X3)) \n","  arr = np.hstack((pred_dt.reshape(-1,1), \n","                  pred_ridge.reshape(-1,1),\n","                  pred_lasso.reshape(-1,1), \n","                  pred_en.reshape(-1,1)))\n","  F3_df = pd.DataFrame(arr, columns=['f3_dt', 'f3_ridge', 'f3_lasso', 'f3_en'], index=X3.index)\n","\n","  #FINAL\n","  test_x = pd.concat([F1_df, F2_df, F3_df], axis=1)\n","  test_x_std = final_scaler.transform(test_x)\n","  test_y_pred = final_ridge.predict(test_x_std)\n","\n","  return test_y_pred"],"execution_count":25,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ke3YWCnxr1g5"},"source":["### Testing"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"COHXrwERtJ3f","executionInfo":{"status":"ok","timestamp":1623236937609,"user_tz":-330,"elapsed":2506,"user":{"displayName":"Kriz Moses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjKBNCwUDTthTPvGUZCLjw9vO9n5ddvIshwxW6L=s64","userId":"11504316763430471705"}},"outputId":"7b894b43-f266-4338-fa6e-1de9dee63421"},"source":["import time\n","start = time.time()\n","\n","predictions = get_search_relevance(test_set[:1])\n","\n","end = time.time()\n","print('Time taken to for {} datapoints = '.format(len(predictions)), round(end-start,3), 'seconds')"],"execution_count":27,"outputs":[{"output_type":"stream","text":["Time taken to for 1 datapoints =  1.984 seconds\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3csbw6LCwqiN","executionInfo":{"status":"ok","timestamp":1623237051859,"user_tz":-330,"elapsed":6428,"user":{"displayName":"Kriz Moses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjKBNCwUDTthTPvGUZCLjw9vO9n5ddvIshwxW6L=s64","userId":"11504316763430471705"}},"outputId":"a76de165-273e-41e0-96fc-439bcc04efe3"},"source":["import time\n","start = time.time()\n","\n","predictions = get_search_relevance(test_set[:100])\n","\n","end = time.time()\n","print('Time taken to for {} datapoints = '.format(len(predictions)), round(end-start,3), 'seconds')"],"execution_count":31,"outputs":[{"output_type":"stream","text":["Time taken to for 100 datapoints =  5.903 seconds\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dNZhi57Ad4-K","executionInfo":{"status":"ok","timestamp":1623237053914,"user_tz":-330,"elapsed":3,"user":{"displayName":"Kriz Moses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjKBNCwUDTthTPvGUZCLjw9vO9n5ddvIshwxW6L=s64","userId":"11504316763430471705"}},"outputId":"14340252-9c46-48eb-a4f5-43b08c9d4cee"},"source":["from sklearn.metrics import mean_squared_error\n","mean_squared_error(test_set_true_relevance[:100], predictions, squared=False)"],"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.4315728818306407"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zPPn4t8cwRpa","executionInfo":{"status":"ok","timestamp":1623141212651,"user_tz":-330,"elapsed":473,"user":{"displayName":"Kriz Moses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjKBNCwUDTthTPvGUZCLjw9vO9n5ddvIshwxW6L=s64","userId":"11504316763430471705"}},"outputId":"8f4c4b13-445f-430e-da04-9c0e900c7a21"},"source":["import time\n","start = time.time()\n","\n","predictions = get_search_relevance(test_set)\n","\n","end = time.time()\n","print('Time taken to for {} datapoints = '.format(len(predictions)), round(end-start,3), 'seconds')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Time taken to for 14814 datapoints =  651.001 seconds\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8ra55N4Usgw1","executionInfo":{"status":"ok","timestamp":1623141236308,"user_tz":-330,"elapsed":486,"user":{"displayName":"Kriz Moses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjKBNCwUDTthTPvGUZCLjw9vO9n5ddvIshwxW6L=s64","userId":"11504316763430471705"}},"outputId":"b14049df-a465-464c-b582-e43f4c1af009"},"source":["from sklearn.metrics import mean_squared_error\n","mean_squared_error(test_set_true_relevance, predictions, squared=False)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.45393187995297146"]},"metadata":{"tags":[]},"execution_count":39}]}]}